{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import deque\n",
    "from replay_memory import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap, wrap_cover, SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_ACTIONS :  4\n",
      "N_STATES :  (4, 84, 84)\n",
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''SAC Settings'''\n",
    "# coefficient of entropy regularization\n",
    "ENT_COEF = 1e-2\n",
    "# experience replay memory size\n",
    "MEMORY_CAPACITY = int(1e+5)\n",
    "# learn start\n",
    "LEARN_START = int(1e+3)\n",
    "# learn frequency\n",
    "LEARN_FREQ = 4\n",
    "\n",
    "'''Environment Settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "# number of environments for SAC\n",
    "N_ENVS = 16\n",
    "# define gym \n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "# check gym setting\n",
    "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ',N_ACTIONS) #  4\n",
    "N_STATES = env.observation_space.shape;print('N_STATES : ',N_STATES) # (4, 84, 84)\n",
    "# Total simulation step\n",
    "N_STEP = int(1e+7)\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# zero-centered gradient penalty\n",
    "ZERO_GP = True\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# log frequency\n",
    "LOG_FREQ = int(1e+3)\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# paths for predction net, target net, result log\n",
    "ACTOR_PATH = './data/model/gail_actor_net.pkl'\n",
    "CRITIC_PATH = './data/model/gail_critic_net.pkl'\n",
    "ACTION_CRITIC_PATH = './data/model/gail_action_critic_net.pkl'\n",
    "DIS_PATH = './data/model/gail_dis_net.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.actor = nn.Linear(256, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_log_prob = F.log_softmax(self.actor(x), dim=1)\n",
    "\n",
    "        return action_log_prob\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))\n",
    "        \n",
    "class CriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))\n",
    "        \n",
    "class ActionCriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionCriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.action_critic = nn.Linear(256, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        action_value = self.action_critic(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))\n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 256)\n",
    "        self.action_feature = nn.Linear(N_ACTIONS, 256)\n",
    "        # actor\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        a_onehot = torch.zeros(x.size(0), N_ACTIONS)\n",
    "        if USE_GPU:\n",
    "            a_onehot = a_onehot.cuda(device=0)\n",
    "        a_onehot.scatter_(1, a.unsqueeze(1), 1)\n",
    "        x = F.relu(self.fc1(x) * F.leaky_relu(self.action_feature(a_onehot), negative_slope=2e-1))\n",
    "        action_value = self.fc2(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self):\n",
    "        self.actor_net = ActorConvNet()\n",
    "        self.critic_net = CriticConvNet()\n",
    "        self.critic_target = CriticConvNet()\n",
    "        self.action_critic_net = ActionCriticConvNet()\n",
    "        self.dis_net = Discriminator()\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.actor_net = self.actor_net.cuda(device=0)\n",
    "            self.critic_net = self.critic_net.cuda(device=0)\n",
    "            # critic target network for stability\n",
    "            self.critic_target = self.critic_net.cuda(device=0)\n",
    "            self.action_critic_net = self.action_critic_net.cuda(device=0)\n",
    "            self.dis_net = self.dis_net.cuda(device=0)\n",
    "        \n",
    "        # sync net and target\n",
    "        self.critic_target.load_state_dict(self.critic_net.state_dict())\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # Create the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        \n",
    "        # create the replay buffer for expert\n",
    "        with open( \"replay.pkl\", \"rb\" ) as f:\n",
    "            self.expert_replay_buffer = pickle.load(f)\n",
    "            \n",
    "        # define optimizer\n",
    "        self.actor_opt = torch.optim.Adam(self.actor_net.parameters(), lr=LR)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic_net.parameters(), lr=LR)\n",
    "        self.action_critic_opt = torch.optim.Adam(self.action_critic_net.parameters(), lr=LR)\n",
    "        self.dis_opt = torch.optim.RMSprop(self.dis_net.parameters(), lr=LR, alpha=0.9)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        self.dis_net.cpu()\n",
    "        \n",
    "        self.actor_net.save(ACTOR_PATH)\n",
    "        self.critic_net.save(CRITIC_PATH)\n",
    "        self.action_critic_net.save(ACTION_CRITIC_PATH)\n",
    "        self.dis_net.save(DIS_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=0)\n",
    "            self.critic_net.cuda(device=0)\n",
    "            self.action_critic_net.cuda(device=0)\n",
    "            self.dis_net.cuda(device=0)\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        self.dis_net.cpu()\n",
    "        \n",
    "        self.actor_net.load(ACTOR_PATH)\n",
    "        self.critic_net.load(CRITIC_PATH)\n",
    "        self.action_critic_net.load(ACTION_CRITIC_PATH)\n",
    "        self.dis_net.load(DIS_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=0)\n",
    "            self.critic_net.cuda(device=0)\n",
    "            self.action_critic_net.cuda(device=0)\n",
    "            self.dis_net.cuda(device=0)\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda(device=0)\n",
    "        # get action log probs and state values\n",
    "        action_log_prob = self.actor_net(x)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1).data.cpu().numpy()\n",
    "        # sample actions\n",
    "        action = np.array([np.random.choice(N_ACTIONS,p=action_prob[i]) for i in range(len(action_prob))])\n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "        \n",
    "    def reward_dis(self, s, a):\n",
    "        d_reward = -torch.log(torch.sigmoid(self.dis_net(s, a)))+torch.log(1-torch.sigmoid(self.dis_net(s, a)))\n",
    "#         d_reward = d_reward.gather(1, a.unsqueeze(1)).squeeze(1) # (N_ENVS)\n",
    "        \n",
    "        return d_reward.detach()\n",
    "        \n",
    "    def learn_dis(self):\n",
    "        # optimize discriminator\n",
    "        \n",
    "        # data sample from experience replay\n",
    "        b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "        b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_s.requires_grad = True\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        \n",
    "        # data sample from expert experience replay\n",
    "        e_state_memory, e_action_memory, e_reward_memory, \\\n",
    "        e_next_state_memory, e_done = self.expert_replay_buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        e_s = torch.FloatTensor(e_state_memory)\n",
    "        e_s.requires_grad = True\n",
    "        e_a = torch.LongTensor(e_action_memory)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            b_s, b_a, e_s, e_a = b_s.cuda(device=0), b_a.cuda(device=0), e_s.cuda(device=0), e_a.cuda(device=0)\n",
    "        \n",
    "        d_policy = self.dis_net(b_s, b_a).squeeze(1) # (m)\n",
    "#         d_policy = d_policy.gather(1, b_a.unsqueeze(1)).squeeze(1) # (m)\n",
    "        d_expert = self.dis_net(e_s, e_a).squeeze(1) # (m)\n",
    "#         d_expert = d_expert.gather(1, e_a.unsqueeze(1)).squeeze(1) # (m)\n",
    "        \n",
    "        d_loss = -torch.log(torch.sigmoid(d_policy)).mean() -torch.log(1-torch.sigmoid(d_expert)).mean()\n",
    "        loss = d_loss\n",
    "        \n",
    "        # calc gradient penalty\n",
    "        if ZERO_GP:\n",
    "            b_grad = autograd.grad(d_policy, b_s, create_graph=True,\n",
    "                            grad_outputs=torch.ones_like(d_policy),\n",
    "                            retain_graph=True, only_inputs=True)[0].view(BATCH_SIZE, -1)\n",
    "            e_grad = autograd.grad(d_expert, e_s, create_graph=True,\n",
    "                            grad_outputs=torch.ones_like(d_expert),\n",
    "                            retain_graph=True, only_inputs=True)[0].view(BATCH_SIZE, -1)\n",
    "            b_grad = b_grad.norm(dim=1)\n",
    "            e_grad = e_grad.norm(dim=1)\n",
    "            gp_loss = 1e+5 * ((b_grad)**2 + (e_grad)**2).mean()\n",
    "            loss += gp_loss\n",
    "        \n",
    "        self.dis_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.dis_opt.step()\n",
    "        \n",
    "        return round(float(d_loss.item()), 4), round(float(gp_loss.item()), 4)\n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        # data sample from experience replay\n",
    "        b_state_memory, b_action_memory, _, \\\n",
    "        b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            \n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_s_ = torch.FloatTensor(b_next_state_memory)\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_s_, b_d = b_s.cuda(device=0), b_a.cuda(device=0), b_s_.cuda(device=0), b_d.cuda(device=0)\n",
    "            \n",
    "        # forward calc\n",
    "        b_r = self.reward_dis(b_s, b_a)\n",
    "        action_log_prob = self.actor_net(b_s)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1)\n",
    "        action_log_prob = F.log_softmax(action_log_prob, dim=1)\n",
    "        cur_value = self.critic_net(b_s).squeeze(1)\n",
    "        next_value = self.critic_target(b_s_)\n",
    "        action_value = self.action_critic_net(b_s)\n",
    "\n",
    "        # critic loss. eq (5) in SAC paper\n",
    "        value_target = (action_value - ENT_COEF * action_log_prob).gather(1, b_a.unsqueeze(1)).squeeze(1)\n",
    "        critic_loss = 0.5 * F.smooth_l1_loss(cur_value, value_target.detach())\n",
    "\n",
    "        # action critic loss. eq (7), (8) in SAC paper\n",
    "        action_value_target = b_r + GAMMA * (1-b_d) * next_value.squeeze(1)\n",
    "        action_critic_loss = 0.5 * F.smooth_l1_loss(action_value.gather(1, \n",
    "            b_a.unsqueeze(1)).squeeze(1), action_value_target.detach())\n",
    "\n",
    "        # actor loss. eq (10) in SAC paper\n",
    "        actor_loss = torch.mean(action_prob*(action_log_prob \\\n",
    "            - F.log_softmax(action_value.detach()/ENT_COEF, dim=1)))\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "        \n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        self.action_critic_opt.zero_grad()\n",
    "        action_critic_loss.backward()\n",
    "        self.action_critic_opt.step()\n",
    "        \n",
    "        self.update_target(self.critic_target, self.critic_net, 1e-3)\n",
    "        \n",
    "        return round(float(actor_loss), 4), round(float(critic_loss), 4), round(float(action_critic_loss), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27231449d944471bbe79f9e1d8b9c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=625000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 16000 | Mean ep 100 return:  1.4 | Used Time: 8.21 | Dis loss :  2.2339 | GP loss :  0.8479\n",
      "Used Step: 32000 | Mean ep 100 return:  1.33 | Used Time: 25.22 | Dis loss :  1.0381 | GP loss :  0.2841\n",
      "Used Step: 48000 | Mean ep 100 return:  1.25 | Used Time: 42.21 | Dis loss :  0.8295 | GP loss :  0.1745\n",
      "Used Step: 64000 | Mean ep 100 return:  1.53 | Used Time: 59.13 | Dis loss :  0.7505 | GP loss :  0.1786\n",
      "Used Step: 80000 | Mean ep 100 return:  1.32 | Used Time: 76.16 | Dis loss :  0.8104 | GP loss :  0.1345\n",
      "Used Step: 96000 | Mean ep 100 return:  1.32 | Used Time: 93.19 | Dis loss :  0.7718 | GP loss :  0.1785\n",
      "Used Step: 112000 | Mean ep 100 return:  1.33 | Used Time: 110.17 | Dis loss :  0.6951 | GP loss :  0.178\n",
      "Used Step: 128000 | Mean ep 100 return:  1.33 | Used Time: 127.22 | Dis loss :  0.6716 | GP loss :  0.1464\n",
      "Used Step: 144000 | Mean ep 100 return:  1.4 | Used Time: 144.2 | Dis loss :  0.6678 | GP loss :  0.1681\n",
      "Used Step: 160000 | Mean ep 100 return:  1.12 | Used Time: 161.18 | Dis loss :  0.8146 | GP loss :  0.1897\n",
      "Used Step: 176000 | Mean ep 100 return:  1.34 | Used Time: 178.09 | Dis loss :  0.5467 | GP loss :  0.1786\n",
      "Used Step: 192000 | Mean ep 100 return:  1.27 | Used Time: 195.25 | Dis loss :  0.5509 | GP loss :  0.1633\n",
      "Used Step: 208000 | Mean ep 100 return:  1.43 | Used Time: 212.16 | Dis loss :  0.6547 | GP loss :  0.1519\n",
      "Used Step: 224000 | Mean ep 100 return:  1.5 | Used Time: 229.04 | Dis loss :  0.6215 | GP loss :  0.1575\n",
      "Used Step: 240000 | Mean ep 100 return:  1.49 | Used Time: 245.86 | Dis loss :  0.774 | GP loss :  0.1668\n",
      "Used Step: 256000 | Mean ep 100 return:  1.56 | Used Time: 262.74 | Dis loss :  0.5986 | GP loss :  0.1833\n",
      "Used Step: 272000 | Mean ep 100 return:  1.36 | Used Time: 279.71 | Dis loss :  0.5972 | GP loss :  0.1784\n",
      "Used Step: 288000 | Mean ep 100 return:  1.56 | Used Time: 296.56 | Dis loss :  0.5922 | GP loss :  0.1373\n",
      "Used Step: 304000 | Mean ep 100 return:  1.26 | Used Time: 313.63 | Dis loss :  0.5226 | GP loss :  0.1587\n",
      "Used Step: 320000 | Mean ep 100 return:  1.21 | Used Time: 330.82 | Dis loss :  0.6684 | GP loss :  0.1831\n",
      "Used Step: 336000 | Mean ep 100 return:  1.24 | Used Time: 347.93 | Dis loss :  0.5557 | GP loss :  0.1443\n",
      "Used Step: 352000 | Mean ep 100 return:  1.18 | Used Time: 364.99 | Dis loss :  0.6482 | GP loss :  0.1457\n",
      "Used Step: 368000 | Mean ep 100 return:  1.47 | Used Time: 381.96 | Dis loss :  0.4814 | GP loss :  0.1692\n",
      "Used Step: 384000 | Mean ep 100 return:  1.48 | Used Time: 398.94 | Dis loss :  0.6515 | GP loss :  0.1618\n",
      "Used Step: 400000 | Mean ep 100 return:  1.39 | Used Time: 415.78 | Dis loss :  0.6126 | GP loss :  0.1466\n",
      "Used Step: 416000 | Mean ep 100 return:  1.55 | Used Time: 432.89 | Dis loss :  0.4828 | GP loss :  0.1495\n",
      "Used Step: 432000 | Mean ep 100 return:  1.09 | Used Time: 450.04 | Dis loss :  0.6152 | GP loss :  0.1572\n",
      "Used Step: 448000 | Mean ep 100 return:  1.98 | Used Time: 466.75 | Dis loss :  0.6451 | GP loss :  0.1611\n",
      "Used Step: 464000 | Mean ep 100 return:  2.1 | Used Time: 483.43 | Dis loss :  0.4969 | GP loss :  0.1865\n",
      "Used Step: 480000 | Mean ep 100 return:  2.14 | Used Time: 500.38 | Dis loss :  0.4941 | GP loss :  0.1475\n",
      "Used Step: 496000 | Mean ep 100 return:  1.3 | Used Time: 517.54 | Dis loss :  0.5703 | GP loss :  0.1444\n",
      "Used Step: 512000 | Mean ep 100 return:  1.63 | Used Time: 534.34 | Dis loss :  0.6654 | GP loss :  0.1753\n",
      "Used Step: 528000 | Mean ep 100 return:  2.06 | Used Time: 551.0 | Dis loss :  0.5497 | GP loss :  0.1204\n",
      "Used Step: 544000 | Mean ep 100 return:  1.7 | Used Time: 567.94 | Dis loss :  0.4744 | GP loss :  0.1435\n",
      "Used Step: 560000 | Mean ep 100 return:  2.3 | Used Time: 584.85 | Dis loss :  0.6699 | GP loss :  0.1311\n",
      "Used Step: 576000 | Mean ep 100 return:  2.04 | Used Time: 601.73 | Dis loss :  0.4737 | GP loss :  0.1264\n",
      "Used Step: 592000 | Mean ep 100 return:  1.78 | Used Time: 618.57 | Dis loss :  0.5353 | GP loss :  0.163\n",
      "Used Step: 608000 | Mean ep 100 return:  2.15 | Used Time: 635.39 | Dis loss :  0.6008 | GP loss :  0.163\n",
      "Used Step: 624000 | Mean ep 100 return:  2.28 | Used Time: 652.32 | Dis loss :  0.4329 | GP loss :  0.145\n",
      "Used Step: 640000 | Mean ep 100 return:  2.14 | Used Time: 669.08 | Dis loss :  0.4858 | GP loss :  0.1385\n",
      "Used Step: 656000 | Mean ep 100 return:  2.1 | Used Time: 685.85 | Dis loss :  0.6311 | GP loss :  0.1379\n",
      "Used Step: 672000 | Mean ep 100 return:  1.57 | Used Time: 702.72 | Dis loss :  0.5233 | GP loss :  0.1555\n",
      "Used Step: 688000 | Mean ep 100 return:  1.69 | Used Time: 719.85 | Dis loss :  0.389 | GP loss :  0.1323\n",
      "Used Step: 704000 | Mean ep 100 return:  1.37 | Used Time: 736.83 | Dis loss :  0.5748 | GP loss :  0.1707\n",
      "Used Step: 720000 | Mean ep 100 return:  1.93 | Used Time: 753.59 | Dis loss :  0.5081 | GP loss :  0.1592\n",
      "Used Step: 736000 | Mean ep 100 return:  2.44 | Used Time: 770.48 | Dis loss :  0.5713 | GP loss :  0.1417\n",
      "Used Step: 752000 | Mean ep 100 return:  1.91 | Used Time: 787.33 | Dis loss :  0.5952 | GP loss :  0.1632\n",
      "Used Step: 768000 | Mean ep 100 return:  2.12 | Used Time: 804.08 | Dis loss :  0.5067 | GP loss :  0.1619\n",
      "Used Step: 784000 | Mean ep 100 return:  1.47 | Used Time: 821.11 | Dis loss :  0.3795 | GP loss :  0.1067\n",
      "Used Step: 800000 | Mean ep 100 return:  1.57 | Used Time: 838.12 | Dis loss :  0.5256 | GP loss :  0.1288\n",
      "Used Step: 816000 | Mean ep 100 return:  1.76 | Used Time: 854.82 | Dis loss :  0.2703 | GP loss :  0.1171\n",
      "Used Step: 832000 | Mean ep 100 return:  2.26 | Used Time: 871.51 | Dis loss :  0.2474 | GP loss :  0.0977\n",
      "Used Step: 848000 | Mean ep 100 return:  1.97 | Used Time: 888.35 | Dis loss :  0.2248 | GP loss :  0.094\n",
      "Used Step: 864000 | Mean ep 100 return:  1.69 | Used Time: 905.27 | Dis loss :  0.1804 | GP loss :  0.0926\n",
      "Used Step: 880000 | Mean ep 100 return:  1.41 | Used Time: 922.25 | Dis loss :  0.246 | GP loss :  0.0637\n",
      "Used Step: 896000 | Mean ep 100 return:  1.89 | Used Time: 939.02 | Dis loss :  0.2972 | GP loss :  0.062\n",
      "Used Step: 912000 | Mean ep 100 return:  2.19 | Used Time: 955.78 | Dis loss :  0.414 | GP loss :  0.087\n",
      "Used Step: 928000 | Mean ep 100 return:  1.94 | Used Time: 972.66 | Dis loss :  0.2422 | GP loss :  0.0642\n",
      "Used Step: 944000 | Mean ep 100 return:  1.81 | Used Time: 989.48 | Dis loss :  0.2118 | GP loss :  0.0711\n",
      "Used Step: 960000 | Mean ep 100 return:  2.22 | Used Time: 1006.12 | Dis loss :  0.3215 | GP loss :  0.1164\n",
      "Used Step: 976000 | Mean ep 100 return:  1.75 | Used Time: 1022.98 | Dis loss :  0.215 | GP loss :  0.0709\n",
      "Used Step: 992000 | Mean ep 100 return:  1.42 | Used Time: 1040.01 | Dis loss :  0.2144 | GP loss :  0.0905\n",
      "Used Step: 1008000 | Mean ep 100 return:  1.57 | Used Time: 1056.95 | Dis loss :  0.1778 | GP loss :  0.0774\n",
      "Used Step: 1024000 | Mean ep 100 return:  1.52 | Used Time: 1074.04 | Dis loss :  0.1866 | GP loss :  0.0588\n",
      "Used Step: 1040000 | Mean ep 100 return:  1.17 | Used Time: 1091.25 | Dis loss :  0.0967 | GP loss :  0.0418\n",
      "Used Step: 1056000 | Mean ep 100 return:  1.3 | Used Time: 1108.4 | Dis loss :  0.2319 | GP loss :  0.0831\n",
      "Used Step: 1072000 | Mean ep 100 return:  1.48 | Used Time: 1125.42 | Dis loss :  0.5351 | GP loss :  0.123\n",
      "Used Step: 1088000 | Mean ep 100 return:  1.98 | Used Time: 1142.2 | Dis loss :  0.2464 | GP loss :  0.0758\n",
      "Used Step: 1104000 | Mean ep 100 return:  2.19 | Used Time: 1159.02 | Dis loss :  0.3388 | GP loss :  0.0423\n",
      "Used Step: 1120000 | Mean ep 100 return:  1.87 | Used Time: 1175.96 | Dis loss :  0.1843 | GP loss :  0.0589\n",
      "Used Step: 1136000 | Mean ep 100 return:  1.9 | Used Time: 1192.71 | Dis loss :  0.2605 | GP loss :  0.0773\n",
      "Used Step: 1152000 | Mean ep 100 return:  1.87 | Used Time: 1209.45 | Dis loss :  0.2495 | GP loss :  0.0778\n",
      "Used Step: 1168000 | Mean ep 100 return:  1.61 | Used Time: 1226.49 | Dis loss :  0.1253 | GP loss :  0.0425\n",
      "Used Step: 1184000 | Mean ep 100 return:  1.69 | Used Time: 1243.4 | Dis loss :  0.1474 | GP loss :  0.0537\n",
      "Used Step: 1200000 | Mean ep 100 return:  1.73 | Used Time: 1260.39 | Dis loss :  0.219 | GP loss :  0.0894\n",
      "Used Step: 1216000 | Mean ep 100 return:  1.54 | Used Time: 1277.32 | Dis loss :  0.2155 | GP loss :  0.0722\n",
      "Used Step: 1232000 | Mean ep 100 return:  1.6 | Used Time: 1294.28 | Dis loss :  0.2795 | GP loss :  0.061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 1248000 | Mean ep 100 return:  1.78 | Used Time: 1311.17 | Dis loss :  0.2527 | GP loss :  0.0741\n",
      "Used Step: 1264000 | Mean ep 100 return:  2.21 | Used Time: 1327.83 | Dis loss :  0.1431 | GP loss :  0.0556\n",
      "Used Step: 1280000 | Mean ep 100 return:  2.84 | Used Time: 1344.49 | Dis loss :  0.1521 | GP loss :  0.0657\n",
      "Used Step: 1296000 | Mean ep 100 return:  2.47 | Used Time: 1361.25 | Dis loss :  0.3873 | GP loss :  0.0919\n",
      "Used Step: 1312000 | Mean ep 100 return:  2.38 | Used Time: 1377.93 | Dis loss :  0.4044 | GP loss :  0.1093\n",
      "Used Step: 1328000 | Mean ep 100 return:  2.14 | Used Time: 1394.68 | Dis loss :  0.1599 | GP loss :  0.0685\n",
      "Used Step: 1344000 | Mean ep 100 return:  2.06 | Used Time: 1411.56 | Dis loss :  0.1546 | GP loss :  0.0658\n",
      "Used Step: 1360000 | Mean ep 100 return:  2.16 | Used Time: 1428.33 | Dis loss :  0.2065 | GP loss :  0.0805\n",
      "Used Step: 1376000 | Mean ep 100 return:  2.42 | Used Time: 1444.95 | Dis loss :  0.2747 | GP loss :  0.0597\n",
      "Used Step: 1392000 | Mean ep 100 return:  2.28 | Used Time: 1461.71 | Dis loss :  0.2683 | GP loss :  0.0699\n",
      "Used Step: 1408000 | Mean ep 100 return:  1.79 | Used Time: 1478.68 | Dis loss :  0.1768 | GP loss :  0.0748\n",
      "Used Step: 1424000 | Mean ep 100 return:  2.26 | Used Time: 1495.32 | Dis loss :  0.1621 | GP loss :  0.0578\n",
      "Used Step: 1440000 | Mean ep 100 return:  2.35 | Used Time: 1512.05 | Dis loss :  0.0905 | GP loss :  0.036\n",
      "Used Step: 1456000 | Mean ep 100 return:  1.25 | Used Time: 1529.07 | Dis loss :  0.2863 | GP loss :  0.0809\n",
      "Used Step: 1472000 | Mean ep 100 return:  1.93 | Used Time: 1545.93 | Dis loss :  0.1725 | GP loss :  0.0376\n",
      "Used Step: 1488000 | Mean ep 100 return:  2.37 | Used Time: 1562.72 | Dis loss :  0.1651 | GP loss :  0.0614\n",
      "Used Step: 1504000 | Mean ep 100 return:  2.3 | Used Time: 1579.43 | Dis loss :  0.2384 | GP loss :  0.0774\n",
      "Used Step: 1520000 | Mean ep 100 return:  2.66 | Used Time: 1595.96 | Dis loss :  0.2037 | GP loss :  0.0492\n",
      "Used Step: 1536000 | Mean ep 100 return:  1.89 | Used Time: 1612.88 | Dis loss :  0.3132 | GP loss :  0.1001\n",
      "Used Step: 1552000 | Mean ep 100 return:  1.9 | Used Time: 1629.72 | Dis loss :  0.1761 | GP loss :  0.0652\n",
      "Used Step: 1568000 | Mean ep 100 return:  1.92 | Used Time: 1646.46 | Dis loss :  0.1133 | GP loss :  0.0386\n",
      "Used Step: 1584000 | Mean ep 100 return:  1.99 | Used Time: 1663.27 | Dis loss :  0.1492 | GP loss :  0.0695\n",
      "Used Step: 1600000 | Mean ep 100 return:  2.25 | Used Time: 1680.0 | Dis loss :  0.2374 | GP loss :  0.0777\n",
      "Used Step: 1616000 | Mean ep 100 return:  2.52 | Used Time: 1696.52 | Dis loss :  0.2986 | GP loss :  0.0887\n",
      "Used Step: 1632000 | Mean ep 100 return:  2.26 | Used Time: 1713.24 | Dis loss :  0.3198 | GP loss :  0.1207\n",
      "Used Step: 1648000 | Mean ep 100 return:  1.32 | Used Time: 1730.33 | Dis loss :  0.2449 | GP loss :  0.0863\n",
      "Used Step: 1664000 | Mean ep 100 return:  1.3 | Used Time: 1747.65 | Dis loss :  0.411 | GP loss :  0.1384\n",
      "Used Step: 1680000 | Mean ep 100 return:  1.39 | Used Time: 1765.1 | Dis loss :  0.3073 | GP loss :  0.1059\n",
      "Used Step: 1696000 | Mean ep 100 return:  1.33 | Used Time: 1782.23 | Dis loss :  0.4851 | GP loss :  0.133\n",
      "Used Step: 1712000 | Mean ep 100 return:  1.26 | Used Time: 1799.54 | Dis loss :  0.3315 | GP loss :  0.0778\n",
      "Used Step: 1728000 | Mean ep 100 return:  1.27 | Used Time: 1816.93 | Dis loss :  0.2819 | GP loss :  0.0783\n",
      "Used Step: 1744000 | Mean ep 100 return:  1.28 | Used Time: 1834.02 | Dis loss :  0.1258 | GP loss :  0.058\n",
      "Used Step: 1760000 | Mean ep 100 return:  1.25 | Used Time: 1851.29 | Dis loss :  0.1075 | GP loss :  0.0601\n",
      "Used Step: 1776000 | Mean ep 100 return:  1.2 | Used Time: 1868.68 | Dis loss :  0.2595 | GP loss :  0.0638\n",
      "Used Step: 1792000 | Mean ep 100 return:  1.35 | Used Time: 1885.8 | Dis loss :  0.1398 | GP loss :  0.0631\n",
      "Used Step: 1808000 | Mean ep 100 return:  1.65 | Used Time: 1902.77 | Dis loss :  0.1578 | GP loss :  0.0342\n",
      "Used Step: 1824000 | Mean ep 100 return:  1.51 | Used Time: 1919.9 | Dis loss :  0.0782 | GP loss :  0.0372\n",
      "Used Step: 1840000 | Mean ep 100 return:  1.04 | Used Time: 1937.27 | Dis loss :  0.1442 | GP loss :  0.0546\n",
      "Used Step: 1856000 | Mean ep 100 return:  0.81 | Used Time: 1954.48 | Dis loss :  0.1209 | GP loss :  0.0493\n",
      "Used Step: 1872000 | Mean ep 100 return:  1.2 | Used Time: 1971.6 | Dis loss :  0.0913 | GP loss :  0.0471\n",
      "Used Step: 1888000 | Mean ep 100 return:  1.51 | Used Time: 1988.78 | Dis loss :  0.1458 | GP loss :  0.0656\n",
      "Used Step: 1904000 | Mean ep 100 return:  1.53 | Used Time: 2005.96 | Dis loss :  0.3728 | GP loss :  0.094\n",
      "Used Step: 1920000 | Mean ep 100 return:  1.46 | Used Time: 2023.13 | Dis loss :  0.1691 | GP loss :  0.0572\n",
      "Used Step: 1936000 | Mean ep 100 return:  1.76 | Used Time: 2040.1 | Dis loss :  0.2307 | GP loss :  0.076\n",
      "Used Step: 1952000 | Mean ep 100 return:  1.24 | Used Time: 2057.23 | Dis loss :  0.3313 | GP loss :  0.102\n",
      "Used Step: 1968000 | Mean ep 100 return:  0.83 | Used Time: 2074.58 | Dis loss :  0.1714 | GP loss :  0.0746\n",
      "Used Step: 1984000 | Mean ep 100 return:  0.95 | Used Time: 2091.87 | Dis loss :  0.1549 | GP loss :  0.0638\n",
      "Used Step: 2000000 | Mean ep 100 return:  1.31 | Used Time: 2109.01 | Dis loss :  0.2805 | GP loss :  0.0868\n",
      "Used Step: 2016000 | Mean ep 100 return:  1.21 | Used Time: 2126.31 | Dis loss :  0.1363 | GP loss :  0.0424\n",
      "Used Step: 2032000 | Mean ep 100 return:  1.97 | Used Time: 2143.42 | Dis loss :  0.1912 | GP loss :  0.0927\n",
      "Used Step: 2048000 | Mean ep 100 return:  1.8 | Used Time: 2160.68 | Dis loss :  0.3013 | GP loss :  0.1023\n",
      "Used Step: 2064000 | Mean ep 100 return:  1.37 | Used Time: 2177.77 | Dis loss :  0.2095 | GP loss :  0.0793\n",
      "Used Step: 2080000 | Mean ep 100 return:  1.32 | Used Time: 2195.03 | Dis loss :  0.127 | GP loss :  0.0549\n",
      "Used Step: 2096000 | Mean ep 100 return:  1.58 | Used Time: 2212.35 | Dis loss :  0.2273 | GP loss :  0.0668\n",
      "Used Step: 2112000 | Mean ep 100 return:  1.51 | Used Time: 2229.59 | Dis loss :  0.315 | GP loss :  0.076\n",
      "Used Step: 2128000 | Mean ep 100 return:  1.55 | Used Time: 2246.68 | Dis loss :  0.0966 | GP loss :  0.0503\n",
      "Used Step: 2144000 | Mean ep 100 return:  1.63 | Used Time: 2263.94 | Dis loss :  0.2083 | GP loss :  0.0859\n",
      "Used Step: 2160000 | Mean ep 100 return:  1.84 | Used Time: 2281.12 | Dis loss :  0.1463 | GP loss :  0.0486\n",
      "Used Step: 2176000 | Mean ep 100 return:  2.09 | Used Time: 2298.17 | Dis loss :  0.1222 | GP loss :  0.0442\n",
      "Used Step: 2192000 | Mean ep 100 return:  1.84 | Used Time: 2315.25 | Dis loss :  0.1438 | GP loss :  0.0605\n",
      "Used Step: 2208000 | Mean ep 100 return:  1.24 | Used Time: 2332.61 | Dis loss :  0.2028 | GP loss :  0.0517\n",
      "Used Step: 2224000 | Mean ep 100 return:  1.04 | Used Time: 2349.85 | Dis loss :  0.1449 | GP loss :  0.0654\n",
      "Used Step: 2240000 | Mean ep 100 return:  1.13 | Used Time: 2367.0 | Dis loss :  0.1853 | GP loss :  0.0707\n",
      "Used Step: 2256000 | Mean ep 100 return:  1.04 | Used Time: 2384.25 | Dis loss :  0.2596 | GP loss :  0.0808\n",
      "Used Step: 2272000 | Mean ep 100 return:  1.47 | Used Time: 2401.55 | Dis loss :  0.194 | GP loss :  0.0505\n",
      "Used Step: 2288000 | Mean ep 100 return:  1.42 | Used Time: 2418.79 | Dis loss :  0.3097 | GP loss :  0.0815\n",
      "Used Step: 2304000 | Mean ep 100 return:  1.53 | Used Time: 2435.87 | Dis loss :  0.0964 | GP loss :  0.0351\n",
      "Used Step: 2320000 | Mean ep 100 return:  0.86 | Used Time: 2453.18 | Dis loss :  0.1013 | GP loss :  0.046\n",
      "Used Step: 2336000 | Mean ep 100 return:  0.72 | Used Time: 2470.52 | Dis loss :  0.2849 | GP loss :  0.0795\n",
      "Used Step: 2352000 | Mean ep 100 return:  1.54 | Used Time: 2487.7 | Dis loss :  0.1174 | GP loss :  0.0389\n",
      "Used Step: 2368000 | Mean ep 100 return:  1.97 | Used Time: 2504.84 | Dis loss :  0.1684 | GP loss :  0.0637\n",
      "Used Step: 2384000 | Mean ep 100 return:  1.15 | Used Time: 2522.1 | Dis loss :  0.1583 | GP loss :  0.051\n",
      "Used Step: 2400000 | Mean ep 100 return:  1.3 | Used Time: 2539.38 | Dis loss :  0.1136 | GP loss :  0.0505\n",
      "Used Step: 2416000 | Mean ep 100 return:  1.16 | Used Time: 2556.58 | Dis loss :  0.2685 | GP loss :  0.0752\n",
      "Used Step: 2432000 | Mean ep 100 return:  1.01 | Used Time: 2573.78 | Dis loss :  0.25 | GP loss :  0.1003\n",
      "Used Step: 2448000 | Mean ep 100 return:  1.05 | Used Time: 2591.11 | Dis loss :  0.1902 | GP loss :  0.0662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 2464000 | Mean ep 100 return:  1.36 | Used Time: 2608.44 | Dis loss :  0.1614 | GP loss :  0.0664\n",
      "Used Step: 2480000 | Mean ep 100 return:  1.36 | Used Time: 2625.51 | Dis loss :  0.2456 | GP loss :  0.0858\n",
      "Used Step: 2496000 | Mean ep 100 return:  1.39 | Used Time: 2642.59 | Dis loss :  0.18 | GP loss :  0.0654\n",
      "Used Step: 2512000 | Mean ep 100 return:  1.38 | Used Time: 2659.74 | Dis loss :  0.251 | GP loss :  0.0889\n",
      "Used Step: 2528000 | Mean ep 100 return:  1.55 | Used Time: 2676.92 | Dis loss :  0.1217 | GP loss :  0.0455\n",
      "Used Step: 2544000 | Mean ep 100 return:  1.08 | Used Time: 2694.17 | Dis loss :  0.2658 | GP loss :  0.0705\n",
      "Used Step: 2560000 | Mean ep 100 return:  0.97 | Used Time: 2711.37 | Dis loss :  0.217 | GP loss :  0.0918\n",
      "Used Step: 2576000 | Mean ep 100 return:  0.94 | Used Time: 2728.6 | Dis loss :  0.2509 | GP loss :  0.0589\n",
      "Used Step: 2592000 | Mean ep 100 return:  0.73 | Used Time: 2746.02 | Dis loss :  0.3316 | GP loss :  0.0998\n",
      "Used Step: 2608000 | Mean ep 100 return:  0.53 | Used Time: 2763.38 | Dis loss :  0.1896 | GP loss :  0.0793\n",
      "Used Step: 2624000 | Mean ep 100 return:  1.21 | Used Time: 2780.58 | Dis loss :  0.263 | GP loss :  0.05\n",
      "Used Step: 2640000 | Mean ep 100 return:  1.29 | Used Time: 2797.8 | Dis loss :  0.1801 | GP loss :  0.0666\n",
      "Used Step: 2656000 | Mean ep 100 return:  1.32 | Used Time: 2814.91 | Dis loss :  0.2275 | GP loss :  0.0526\n",
      "Used Step: 2672000 | Mean ep 100 return:  1.5 | Used Time: 2831.99 | Dis loss :  0.1103 | GP loss :  0.0424\n",
      "Used Step: 2688000 | Mean ep 100 return:  1.15 | Used Time: 2849.32 | Dis loss :  0.1988 | GP loss :  0.0531\n",
      "Used Step: 2704000 | Mean ep 100 return:  1.14 | Used Time: 2866.54 | Dis loss :  0.1926 | GP loss :  0.0756\n",
      "Used Step: 2720000 | Mean ep 100 return:  1.35 | Used Time: 2883.81 | Dis loss :  0.3152 | GP loss :  0.1143\n",
      "Used Step: 2736000 | Mean ep 100 return:  1.26 | Used Time: 2901.03 | Dis loss :  0.1932 | GP loss :  0.0871\n",
      "Used Step: 2752000 | Mean ep 100 return:  1.24 | Used Time: 2918.48 | Dis loss :  0.2388 | GP loss :  0.0766\n",
      "Used Step: 2768000 | Mean ep 100 return:  1.37 | Used Time: 2935.98 | Dis loss :  0.1938 | GP loss :  0.0655\n",
      "Used Step: 2784000 | Mean ep 100 return:  1.31 | Used Time: 2953.33 | Dis loss :  0.1577 | GP loss :  0.0849\n",
      "Used Step: 2800000 | Mean ep 100 return:  1.16 | Used Time: 2970.64 | Dis loss :  0.1952 | GP loss :  0.0686\n",
      "Used Step: 2816000 | Mean ep 100 return:  1.59 | Used Time: 2988.0 | Dis loss :  0.2227 | GP loss :  0.0952\n",
      "Used Step: 2832000 | Mean ep 100 return:  1.95 | Used Time: 3005.07 | Dis loss :  0.1167 | GP loss :  0.0502\n",
      "Used Step: 2848000 | Mean ep 100 return:  1.45 | Used Time: 3022.18 | Dis loss :  0.2717 | GP loss :  0.0993\n",
      "Used Step: 2864000 | Mean ep 100 return:  1.32 | Used Time: 3039.4 | Dis loss :  0.1982 | GP loss :  0.0442\n",
      "Used Step: 2880000 | Mean ep 100 return:  1.48 | Used Time: 3056.56 | Dis loss :  0.0815 | GP loss :  0.0315\n",
      "Used Step: 2896000 | Mean ep 100 return:  1.38 | Used Time: 3073.76 | Dis loss :  0.1673 | GP loss :  0.058\n",
      "Used Step: 2912000 | Mean ep 100 return:  1.52 | Used Time: 3090.77 | Dis loss :  0.2736 | GP loss :  0.0673\n",
      "Used Step: 2928000 | Mean ep 100 return:  1.95 | Used Time: 3107.92 | Dis loss :  0.3064 | GP loss :  0.0591\n",
      "Used Step: 2944000 | Mean ep 100 return:  1.37 | Used Time: 3125.16 | Dis loss :  0.1012 | GP loss :  0.0373\n",
      "Used Step: 2960000 | Mean ep 100 return:  1.4 | Used Time: 3142.33 | Dis loss :  0.2521 | GP loss :  0.0673\n",
      "Used Step: 2976000 | Mean ep 100 return:  1.41 | Used Time: 3159.5 | Dis loss :  0.2495 | GP loss :  0.0881\n",
      "Used Step: 2992000 | Mean ep 100 return:  1.19 | Used Time: 3176.81 | Dis loss :  0.1788 | GP loss :  0.0737\n",
      "Used Step: 3008000 | Mean ep 100 return:  1.33 | Used Time: 3194.12 | Dis loss :  0.1098 | GP loss :  0.0492\n",
      "Used Step: 3024000 | Mean ep 100 return:  1.73 | Used Time: 3211.28 | Dis loss :  0.1663 | GP loss :  0.0812\n",
      "Used Step: 3040000 | Mean ep 100 return:  1.48 | Used Time: 3228.24 | Dis loss :  0.1146 | GP loss :  0.0378\n",
      "Used Step: 3056000 | Mean ep 100 return:  1.71 | Used Time: 3245.16 | Dis loss :  0.2163 | GP loss :  0.0707\n",
      "Used Step: 3072000 | Mean ep 100 return:  1.47 | Used Time: 3262.48 | Dis loss :  0.1205 | GP loss :  0.0463\n",
      "Used Step: 3088000 | Mean ep 100 return:  1.44 | Used Time: 3279.6 | Dis loss :  0.2733 | GP loss :  0.1092\n",
      "Used Step: 3104000 | Mean ep 100 return:  1.04 | Used Time: 3296.74 | Dis loss :  0.2705 | GP loss :  0.083\n",
      "Used Step: 3120000 | Mean ep 100 return:  0.7 | Used Time: 3314.03 | Dis loss :  0.2045 | GP loss :  0.0847\n",
      "Used Step: 3136000 | Mean ep 100 return:  1.13 | Used Time: 3331.27 | Dis loss :  0.2153 | GP loss :  0.0522\n",
      "Used Step: 3152000 | Mean ep 100 return:  1.31 | Used Time: 3348.59 | Dis loss :  0.282 | GP loss :  0.0465\n",
      "Used Step: 3168000 | Mean ep 100 return:  1.53 | Used Time: 3365.81 | Dis loss :  0.0803 | GP loss :  0.0262\n",
      "Used Step: 3184000 | Mean ep 100 return:  1.37 | Used Time: 3382.99 | Dis loss :  0.348 | GP loss :  0.0775\n",
      "Used Step: 3200000 | Mean ep 100 return:  1.7 | Used Time: 3400.29 | Dis loss :  0.1867 | GP loss :  0.0471\n",
      "Used Step: 3216000 | Mean ep 100 return:  1.42 | Used Time: 3417.34 | Dis loss :  0.1964 | GP loss :  0.0596\n",
      "Used Step: 3232000 | Mean ep 100 return:  1.44 | Used Time: 3434.23 | Dis loss :  0.151 | GP loss :  0.0543\n",
      "Used Step: 3248000 | Mean ep 100 return:  1.01 | Used Time: 3451.57 | Dis loss :  0.1869 | GP loss :  0.0579\n",
      "Used Step: 3264000 | Mean ep 100 return:  1.14 | Used Time: 3468.68 | Dis loss :  0.1284 | GP loss :  0.0582\n",
      "Used Step: 3280000 | Mean ep 100 return:  1.36 | Used Time: 3485.71 | Dis loss :  0.1567 | GP loss :  0.0593\n",
      "Used Step: 3296000 | Mean ep 100 return:  1.55 | Used Time: 3502.82 | Dis loss :  0.0855 | GP loss :  0.0447\n",
      "Used Step: 3312000 | Mean ep 100 return:  1.18 | Used Time: 3520.14 | Dis loss :  0.1035 | GP loss :  0.0445\n",
      "Used Step: 3328000 | Mean ep 100 return:  1.3 | Used Time: 3537.45 | Dis loss :  0.2063 | GP loss :  0.0645\n",
      "Used Step: 3344000 | Mean ep 100 return:  1.53 | Used Time: 3554.5 | Dis loss :  0.2731 | GP loss :  0.0862\n",
      "Used Step: 3360000 | Mean ep 100 return:  1.41 | Used Time: 3571.71 | Dis loss :  0.1398 | GP loss :  0.0441\n",
      "Used Step: 3376000 | Mean ep 100 return:  1.32 | Used Time: 3588.95 | Dis loss :  0.2136 | GP loss :  0.0613\n",
      "Used Step: 3392000 | Mean ep 100 return:  1.39 | Used Time: 3606.2 | Dis loss :  0.1934 | GP loss :  0.064\n",
      "Used Step: 3408000 | Mean ep 100 return:  1.65 | Used Time: 3623.15 | Dis loss :  0.1439 | GP loss :  0.0495\n",
      "Used Step: 3424000 | Mean ep 100 return:  1.16 | Used Time: 3640.07 | Dis loss :  0.1438 | GP loss :  0.0755\n",
      "Used Step: 3440000 | Mean ep 100 return:  1.5 | Used Time: 3657.35 | Dis loss :  0.2212 | GP loss :  0.0845\n",
      "Used Step: 3456000 | Mean ep 100 return:  1.64 | Used Time: 3674.59 | Dis loss :  0.1248 | GP loss :  0.044\n",
      "Used Step: 3472000 | Mean ep 100 return:  1.59 | Used Time: 3691.81 | Dis loss :  0.1698 | GP loss :  0.0594\n",
      "Used Step: 3488000 | Mean ep 100 return:  0.88 | Used Time: 3709.03 | Dis loss :  0.2248 | GP loss :  0.0651\n",
      "Used Step: 3504000 | Mean ep 100 return:  0.83 | Used Time: 3726.31 | Dis loss :  0.1014 | GP loss :  0.0333\n",
      "Used Step: 3520000 | Mean ep 100 return:  1.19 | Used Time: 3743.59 | Dis loss :  0.1542 | GP loss :  0.0473\n",
      "Used Step: 3536000 | Mean ep 100 return:  1.19 | Used Time: 3760.69 | Dis loss :  0.1046 | GP loss :  0.0426\n",
      "Used Step: 3552000 | Mean ep 100 return:  1.04 | Used Time: 3777.73 | Dis loss :  0.13 | GP loss :  0.0423\n",
      "Used Step: 3568000 | Mean ep 100 return:  1.2 | Used Time: 3794.81 | Dis loss :  0.1071 | GP loss :  0.0415\n",
      "Used Step: 3584000 | Mean ep 100 return:  1.02 | Used Time: 3812.21 | Dis loss :  0.3109 | GP loss :  0.0885\n",
      "Used Step: 3600000 | Mean ep 100 return:  0.98 | Used Time: 3829.58 | Dis loss :  0.144 | GP loss :  0.0361\n",
      "Used Step: 3616000 | Mean ep 100 return:  1.52 | Used Time: 3846.71 | Dis loss :  0.1688 | GP loss :  0.0879\n",
      "Used Step: 3632000 | Mean ep 100 return:  1.3 | Used Time: 3863.94 | Dis loss :  0.1742 | GP loss :  0.0614\n",
      "Used Step: 3648000 | Mean ep 100 return:  1.42 | Used Time: 3881.27 | Dis loss :  0.18 | GP loss :  0.0693\n",
      "Used Step: 3664000 | Mean ep 100 return:  1.33 | Used Time: 3898.48 | Dis loss :  0.1931 | GP loss :  0.0743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 3680000 | Mean ep 100 return:  1.37 | Used Time: 3915.63 | Dis loss :  0.1685 | GP loss :  0.0709\n",
      "Used Step: 3696000 | Mean ep 100 return:  1.31 | Used Time: 3932.94 | Dis loss :  0.1007 | GP loss :  0.0377\n",
      "Used Step: 3712000 | Mean ep 100 return:  1.15 | Used Time: 3950.32 | Dis loss :  0.3258 | GP loss :  0.0911\n",
      "Used Step: 3728000 | Mean ep 100 return:  1.41 | Used Time: 3967.45 | Dis loss :  0.182 | GP loss :  0.0669\n",
      "Used Step: 3744000 | Mean ep 100 return:  1.37 | Used Time: 3984.64 | Dis loss :  0.1751 | GP loss :  0.0587\n",
      "Used Step: 3760000 | Mean ep 100 return:  1.22 | Used Time: 4001.84 | Dis loss :  0.39 | GP loss :  0.1249\n",
      "Used Step: 3776000 | Mean ep 100 return:  0.92 | Used Time: 4019.24 | Dis loss :  0.1862 | GP loss :  0.0676\n",
      "Used Step: 3792000 | Mean ep 100 return:  1.32 | Used Time: 4036.35 | Dis loss :  0.1528 | GP loss :  0.0512\n",
      "Used Step: 3808000 | Mean ep 100 return:  1.47 | Used Time: 4053.46 | Dis loss :  0.1223 | GP loss :  0.0444\n",
      "Used Step: 3824000 | Mean ep 100 return:  1.27 | Used Time: 4070.56 | Dis loss :  0.1658 | GP loss :  0.0675\n",
      "Used Step: 3840000 | Mean ep 100 return:  1.28 | Used Time: 4087.57 | Dis loss :  0.0559 | GP loss :  0.0254\n",
      "Used Step: 3856000 | Mean ep 100 return:  1.23 | Used Time: 4104.63 | Dis loss :  0.1812 | GP loss :  0.0521\n",
      "Used Step: 3872000 | Mean ep 100 return:  1.25 | Used Time: 4121.77 | Dis loss :  0.1855 | GP loss :  0.0788\n",
      "Used Step: 3888000 | Mean ep 100 return:  1.37 | Used Time: 4138.82 | Dis loss :  0.164 | GP loss :  0.074\n",
      "Used Step: 3904000 | Mean ep 100 return:  1.24 | Used Time: 4155.94 | Dis loss :  0.1028 | GP loss :  0.0481\n",
      "Used Step: 3920000 | Mean ep 100 return:  1.29 | Used Time: 4172.94 | Dis loss :  0.2374 | GP loss :  0.103\n",
      "Used Step: 3936000 | Mean ep 100 return:  1.91 | Used Time: 4189.87 | Dis loss :  0.1987 | GP loss :  0.0935\n",
      "Used Step: 3952000 | Mean ep 100 return:  1.87 | Used Time: 4207.07 | Dis loss :  0.2267 | GP loss :  0.0899\n",
      "Used Step: 3968000 | Mean ep 100 return:  1.25 | Used Time: 4224.17 | Dis loss :  0.223 | GP loss :  0.078\n",
      "Used Step: 3984000 | Mean ep 100 return:  1.22 | Used Time: 4241.29 | Dis loss :  0.0783 | GP loss :  0.0333\n",
      "Used Step: 4000000 | Mean ep 100 return:  0.98 | Used Time: 4258.47 | Dis loss :  0.3077 | GP loss :  0.0877\n",
      "Used Step: 4016000 | Mean ep 100 return:  0.93 | Used Time: 4275.73 | Dis loss :  0.1979 | GP loss :  0.0879\n",
      "Used Step: 4032000 | Mean ep 100 return:  1.1 | Used Time: 4292.86 | Dis loss :  0.145 | GP loss :  0.0476\n",
      "Used Step: 4048000 | Mean ep 100 return:  0.86 | Used Time: 4309.94 | Dis loss :  0.2365 | GP loss :  0.0883\n",
      "Used Step: 4064000 | Mean ep 100 return:  1.64 | Used Time: 4327.09 | Dis loss :  0.1311 | GP loss :  0.0634\n",
      "Used Step: 4080000 | Mean ep 100 return:  1.28 | Used Time: 4344.32 | Dis loss :  0.1545 | GP loss :  0.0528\n",
      "Used Step: 4096000 | Mean ep 100 return:  1.05 | Used Time: 4361.56 | Dis loss :  0.1527 | GP loss :  0.0615\n",
      "Used Step: 4112000 | Mean ep 100 return:  0.77 | Used Time: 4378.76 | Dis loss :  0.1224 | GP loss :  0.0464\n",
      "Used Step: 4128000 | Mean ep 100 return:  0.68 | Used Time: 4396.09 | Dis loss :  0.1138 | GP loss :  0.0451\n",
      "Used Step: 4144000 | Mean ep 100 return:  0.92 | Used Time: 4413.37 | Dis loss :  0.2908 | GP loss :  0.0626\n",
      "Used Step: 4160000 | Mean ep 100 return:  1.06 | Used Time: 4430.47 | Dis loss :  0.1679 | GP loss :  0.0483\n",
      "Used Step: 4176000 | Mean ep 100 return:  1.02 | Used Time: 4447.57 | Dis loss :  0.1329 | GP loss :  0.0536\n",
      "Used Step: 4192000 | Mean ep 100 return:  0.88 | Used Time: 4464.87 | Dis loss :  0.113 | GP loss :  0.0434\n",
      "Used Step: 4208000 | Mean ep 100 return:  0.49 | Used Time: 4482.32 | Dis loss :  0.0744 | GP loss :  0.0326\n",
      "Used Step: 4224000 | Mean ep 100 return:  1.03 | Used Time: 4499.39 | Dis loss :  0.2293 | GP loss :  0.067\n",
      "Used Step: 4240000 | Mean ep 100 return:  1.38 | Used Time: 4516.44 | Dis loss :  0.1367 | GP loss :  0.0535\n",
      "Used Step: 4256000 | Mean ep 100 return:  1.42 | Used Time: 4533.47 | Dis loss :  0.1543 | GP loss :  0.0696\n",
      "Used Step: 4272000 | Mean ep 100 return:  1.33 | Used Time: 4550.78 | Dis loss :  0.1113 | GP loss :  0.0378\n",
      "Used Step: 4288000 | Mean ep 100 return:  1.58 | Used Time: 4567.84 | Dis loss :  0.224 | GP loss :  0.0747\n",
      "Used Step: 4304000 | Mean ep 100 return:  1.61 | Used Time: 4584.7 | Dis loss :  0.2553 | GP loss :  0.0672\n",
      "Used Step: 4320000 | Mean ep 100 return:  1.27 | Used Time: 4601.93 | Dis loss :  0.1049 | GP loss :  0.0424\n",
      "Used Step: 4336000 | Mean ep 100 return:  1.31 | Used Time: 4619.12 | Dis loss :  0.128 | GP loss :  0.0557\n",
      "Used Step: 4352000 | Mean ep 100 return:  1.31 | Used Time: 4636.14 | Dis loss :  0.334 | GP loss :  0.1047\n",
      "Used Step: 4368000 | Mean ep 100 return:  1.08 | Used Time: 4653.18 | Dis loss :  0.2586 | GP loss :  0.1036\n",
      "Used Step: 4384000 | Mean ep 100 return:  0.67 | Used Time: 4670.49 | Dis loss :  0.2961 | GP loss :  0.0829\n",
      "Used Step: 4400000 | Mean ep 100 return:  0.98 | Used Time: 4687.89 | Dis loss :  0.1815 | GP loss :  0.0809\n",
      "Used Step: 4416000 | Mean ep 100 return:  1.35 | Used Time: 4704.96 | Dis loss :  0.1153 | GP loss :  0.0475\n",
      "Used Step: 4432000 | Mean ep 100 return:  0.91 | Used Time: 4722.26 | Dis loss :  0.2367 | GP loss :  0.0749\n",
      "Used Step: 4448000 | Mean ep 100 return:  1.13 | Used Time: 4739.48 | Dis loss :  0.2602 | GP loss :  0.06\n",
      "Used Step: 4464000 | Mean ep 100 return:  1.17 | Used Time: 4756.68 | Dis loss :  0.1302 | GP loss :  0.0574\n",
      "Used Step: 4480000 | Mean ep 100 return:  0.68 | Used Time: 4774.01 | Dis loss :  0.1313 | GP loss :  0.0672\n",
      "Used Step: 4496000 | Mean ep 100 return:  0.79 | Used Time: 4791.44 | Dis loss :  0.1874 | GP loss :  0.0459\n",
      "Used Step: 4512000 | Mean ep 100 return:  1.28 | Used Time: 4808.66 | Dis loss :  0.1037 | GP loss :  0.0392\n",
      "Used Step: 4528000 | Mean ep 100 return:  1.43 | Used Time: 4825.6 | Dis loss :  0.1824 | GP loss :  0.0422\n",
      "Used Step: 4544000 | Mean ep 100 return:  1.52 | Used Time: 4842.49 | Dis loss :  0.0957 | GP loss :  0.0449\n",
      "Used Step: 4560000 | Mean ep 100 return:  1.61 | Used Time: 4859.54 | Dis loss :  0.1341 | GP loss :  0.0401\n",
      "Used Step: 4576000 | Mean ep 100 return:  1.12 | Used Time: 4876.86 | Dis loss :  0.0952 | GP loss :  0.0323\n",
      "Used Step: 4592000 | Mean ep 100 return:  1.08 | Used Time: 4893.99 | Dis loss :  0.1284 | GP loss :  0.0679\n",
      "Used Step: 4608000 | Mean ep 100 return:  1.15 | Used Time: 4911.11 | Dis loss :  0.1943 | GP loss :  0.0761\n",
      "Used Step: 4624000 | Mean ep 100 return:  1.13 | Used Time: 4928.41 | Dis loss :  0.1428 | GP loss :  0.0574\n",
      "Used Step: 4640000 | Mean ep 100 return:  1.44 | Used Time: 4945.49 | Dis loss :  0.2913 | GP loss :  0.0729\n",
      "Used Step: 4656000 | Mean ep 100 return:  1.17 | Used Time: 4962.52 | Dis loss :  0.1777 | GP loss :  0.0577\n",
      "Used Step: 4672000 | Mean ep 100 return:  1.33 | Used Time: 4979.54 | Dis loss :  0.1484 | GP loss :  0.0569\n",
      "Used Step: 4688000 | Mean ep 100 return:  1.08 | Used Time: 4996.95 | Dis loss :  0.1485 | GP loss :  0.0631\n",
      "Used Step: 4704000 | Mean ep 100 return:  0.89 | Used Time: 5014.22 | Dis loss :  0.117 | GP loss :  0.0474\n",
      "Used Step: 4720000 | Mean ep 100 return:  1.25 | Used Time: 5031.38 | Dis loss :  0.1308 | GP loss :  0.0427\n",
      "Used Step: 4736000 | Mean ep 100 return:  1.08 | Used Time: 5048.7 | Dis loss :  0.1069 | GP loss :  0.0529\n",
      "Used Step: 4752000 | Mean ep 100 return:  0.99 | Used Time: 5066.08 | Dis loss :  0.2176 | GP loss :  0.0518\n",
      "Used Step: 4768000 | Mean ep 100 return:  1.49 | Used Time: 5083.3 | Dis loss :  0.2296 | GP loss :  0.088\n",
      "Used Step: 4784000 | Mean ep 100 return:  1.33 | Used Time: 5100.52 | Dis loss :  0.101 | GP loss :  0.0301\n",
      "Used Step: 4800000 | Mean ep 100 return:  1.14 | Used Time: 5117.73 | Dis loss :  0.1946 | GP loss :  0.0907\n",
      "Used Step: 4816000 | Mean ep 100 return:  1.08 | Used Time: 5135.07 | Dis loss :  0.2728 | GP loss :  0.0705\n",
      "Used Step: 4832000 | Mean ep 100 return:  0.91 | Used Time: 5152.34 | Dis loss :  0.0788 | GP loss :  0.0309\n",
      "Used Step: 4848000 | Mean ep 100 return:  1.23 | Used Time: 5169.54 | Dis loss :  0.2137 | GP loss :  0.0788\n",
      "Used Step: 4864000 | Mean ep 100 return:  1.7 | Used Time: 5186.72 | Dis loss :  0.2319 | GP loss :  0.0925\n",
      "Used Step: 4880000 | Mean ep 100 return:  2.32 | Used Time: 5203.8 | Dis loss :  0.2108 | GP loss :  0.047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 4896000 | Mean ep 100 return:  1.21 | Used Time: 5220.92 | Dis loss :  0.2411 | GP loss :  0.0787\n",
      "Used Step: 4912000 | Mean ep 100 return:  1.66 | Used Time: 5238.07 | Dis loss :  0.2083 | GP loss :  0.0625\n",
      "Used Step: 4928000 | Mean ep 100 return:  1.22 | Used Time: 5255.3 | Dis loss :  0.3661 | GP loss :  0.1069\n",
      "Used Step: 4944000 | Mean ep 100 return:  1.5 | Used Time: 5272.52 | Dis loss :  0.1445 | GP loss :  0.0622\n",
      "Used Step: 4960000 | Mean ep 100 return:  1.68 | Used Time: 5289.49 | Dis loss :  0.2058 | GP loss :  0.0728\n",
      "Used Step: 4976000 | Mean ep 100 return:  1.24 | Used Time: 5306.44 | Dis loss :  0.1523 | GP loss :  0.0743\n",
      "Used Step: 4992000 | Mean ep 100 return:  1.12 | Used Time: 5323.62 | Dis loss :  0.2985 | GP loss :  0.1059\n",
      "Used Step: 5008000 | Mean ep 100 return:  1.16 | Used Time: 5340.87 | Dis loss :  0.1587 | GP loss :  0.0729\n",
      "Used Step: 5024000 | Mean ep 100 return:  1.02 | Used Time: 5358.02 | Dis loss :  0.1629 | GP loss :  0.0591\n",
      "Used Step: 5040000 | Mean ep 100 return:  0.79 | Used Time: 5375.23 | Dis loss :  0.1275 | GP loss :  0.061\n",
      "Used Step: 5056000 | Mean ep 100 return:  0.98 | Used Time: 5392.62 | Dis loss :  0.1061 | GP loss :  0.0483\n",
      "Used Step: 5072000 | Mean ep 100 return:  1.15 | Used Time: 5409.91 | Dis loss :  0.0942 | GP loss :  0.0476\n",
      "Used Step: 5088000 | Mean ep 100 return:  1.06 | Used Time: 5427.13 | Dis loss :  0.1384 | GP loss :  0.0525\n",
      "Used Step: 5104000 | Mean ep 100 return:  1.08 | Used Time: 5444.33 | Dis loss :  0.1121 | GP loss :  0.0359\n",
      "Used Step: 5120000 | Mean ep 100 return:  1.24 | Used Time: 5461.53 | Dis loss :  0.1791 | GP loss :  0.0771\n",
      "Used Step: 5136000 | Mean ep 100 return:  1.17 | Used Time: 5478.83 | Dis loss :  0.1408 | GP loss :  0.0519\n",
      "Used Step: 5152000 | Mean ep 100 return:  1.11 | Used Time: 5496.07 | Dis loss :  0.1446 | GP loss :  0.0425\n",
      "Used Step: 5168000 | Mean ep 100 return:  1.18 | Used Time: 5513.16 | Dis loss :  0.2115 | GP loss :  0.0672\n",
      "Used Step: 5184000 | Mean ep 100 return:  0.9 | Used Time: 5530.47 | Dis loss :  0.1547 | GP loss :  0.0735\n",
      "Used Step: 5200000 | Mean ep 100 return:  1.07 | Used Time: 5547.74 | Dis loss :  0.2794 | GP loss :  0.0785\n",
      "Used Step: 5216000 | Mean ep 100 return:  1.19 | Used Time: 5564.86 | Dis loss :  0.3004 | GP loss :  0.0926\n",
      "Used Step: 5232000 | Mean ep 100 return:  1.17 | Used Time: 5581.99 | Dis loss :  0.1664 | GP loss :  0.047\n",
      "Used Step: 5248000 | Mean ep 100 return:  1.23 | Used Time: 5599.28 | Dis loss :  0.2929 | GP loss :  0.0673\n",
      "Used Step: 5264000 | Mean ep 100 return:  1.05 | Used Time: 5616.57 | Dis loss :  0.127 | GP loss :  0.0447\n",
      "Used Step: 5280000 | Mean ep 100 return:  1.36 | Used Time: 5633.72 | Dis loss :  0.3197 | GP loss :  0.1135\n",
      "Used Step: 5296000 | Mean ep 100 return:  1.33 | Used Time: 5650.84 | Dis loss :  0.1981 | GP loss :  0.0455\n",
      "Used Step: 5312000 | Mean ep 100 return:  1.39 | Used Time: 5668.23 | Dis loss :  0.172 | GP loss :  0.0692\n",
      "Used Step: 5328000 | Mean ep 100 return:  1.36 | Used Time: 5685.37 | Dis loss :  0.1491 | GP loss :  0.065\n",
      "Used Step: 5344000 | Mean ep 100 return:  1.68 | Used Time: 5702.44 | Dis loss :  0.1821 | GP loss :  0.0638\n",
      "Used Step: 5360000 | Mean ep 100 return:  1.75 | Used Time: 5719.4 | Dis loss :  0.0931 | GP loss :  0.0312\n",
      "Used Step: 5376000 | Mean ep 100 return:  1.65 | Used Time: 5736.56 | Dis loss :  0.3136 | GP loss :  0.0725\n",
      "Used Step: 5392000 | Mean ep 100 return:  1.4 | Used Time: 5753.86 | Dis loss :  0.1218 | GP loss :  0.0607\n",
      "Used Step: 5408000 | Mean ep 100 return:  1.02 | Used Time: 5771.12 | Dis loss :  0.1083 | GP loss :  0.0495\n",
      "Used Step: 5424000 | Mean ep 100 return:  1.09 | Used Time: 5788.27 | Dis loss :  0.2106 | GP loss :  0.0645\n",
      "Used Step: 5440000 | Mean ep 100 return:  1.27 | Used Time: 5805.5 | Dis loss :  0.1907 | GP loss :  0.0617\n",
      "Used Step: 5456000 | Mean ep 100 return:  1.08 | Used Time: 5822.66 | Dis loss :  0.1219 | GP loss :  0.0482\n",
      "Used Step: 5472000 | Mean ep 100 return:  1.22 | Used Time: 5839.67 | Dis loss :  0.2298 | GP loss :  0.0809\n",
      "Used Step: 5488000 | Mean ep 100 return:  2.38 | Used Time: 5856.71 | Dis loss :  0.139 | GP loss :  0.0502\n",
      "Used Step: 5504000 | Mean ep 100 return:  1.79 | Used Time: 5873.89 | Dis loss :  0.2455 | GP loss :  0.0755\n",
      "Used Step: 5520000 | Mean ep 100 return:  1.56 | Used Time: 5891.15 | Dis loss :  0.082 | GP loss :  0.0317\n",
      "Used Step: 5536000 | Mean ep 100 return:  1.29 | Used Time: 5908.33 | Dis loss :  0.1367 | GP loss :  0.0605\n",
      "Used Step: 5552000 | Mean ep 100 return:  1.78 | Used Time: 5925.19 | Dis loss :  0.0931 | GP loss :  0.0439\n",
      "Used Step: 5568000 | Mean ep 100 return:  1.22 | Used Time: 5942.3 | Dis loss :  0.1197 | GP loss :  0.0431\n",
      "Used Step: 5584000 | Mean ep 100 return:  1.08 | Used Time: 5959.73 | Dis loss :  0.0886 | GP loss :  0.0538\n",
      "Used Step: 5600000 | Mean ep 100 return:  1.16 | Used Time: 5977.17 | Dis loss :  0.2962 | GP loss :  0.0737\n",
      "Used Step: 5616000 | Mean ep 100 return:  1.26 | Used Time: 5994.21 | Dis loss :  0.1245 | GP loss :  0.0435\n",
      "Used Step: 5632000 | Mean ep 100 return:  1.54 | Used Time: 6011.3 | Dis loss :  0.1639 | GP loss :  0.0763\n",
      "Used Step: 5648000 | Mean ep 100 return:  1.62 | Used Time: 6028.5 | Dis loss :  0.2158 | GP loss :  0.0445\n",
      "Used Step: 5664000 | Mean ep 100 return:  1.34 | Used Time: 6045.73 | Dis loss :  0.3251 | GP loss :  0.08\n",
      "Used Step: 5680000 | Mean ep 100 return:  0.83 | Used Time: 6062.93 | Dis loss :  0.1177 | GP loss :  0.0566\n",
      "Used Step: 5696000 | Mean ep 100 return:  0.96 | Used Time: 6080.1 | Dis loss :  0.1597 | GP loss :  0.0608\n",
      "Used Step: 5712000 | Mean ep 100 return:  0.97 | Used Time: 6097.48 | Dis loss :  0.311 | GP loss :  0.0689\n",
      "Used Step: 5728000 | Mean ep 100 return:  1.3 | Used Time: 6114.71 | Dis loss :  0.0879 | GP loss :  0.0318\n",
      "Used Step: 5744000 | Mean ep 100 return:  1.9 | Used Time: 6131.69 | Dis loss :  0.1914 | GP loss :  0.0405\n",
      "Used Step: 5760000 | Mean ep 100 return:  1.24 | Used Time: 6148.93 | Dis loss :  0.2505 | GP loss :  0.0899\n",
      "Used Step: 5776000 | Mean ep 100 return:  0.74 | Used Time: 6166.33 | Dis loss :  0.1134 | GP loss :  0.0522\n",
      "Used Step: 5792000 | Mean ep 100 return:  1.03 | Used Time: 6183.78 | Dis loss :  0.272 | GP loss :  0.0885\n",
      "Used Step: 5808000 | Mean ep 100 return:  1.56 | Used Time: 6200.87 | Dis loss :  0.1937 | GP loss :  0.0757\n",
      "Used Step: 5824000 | Mean ep 100 return:  1.65 | Used Time: 6217.9 | Dis loss :  0.098 | GP loss :  0.0425\n",
      "Used Step: 5840000 | Mean ep 100 return:  1.41 | Used Time: 6235.11 | Dis loss :  0.0953 | GP loss :  0.0393\n",
      "Used Step: 5856000 | Mean ep 100 return:  1.47 | Used Time: 6252.44 | Dis loss :  0.3518 | GP loss :  0.0714\n",
      "Used Step: 5872000 | Mean ep 100 return:  1.2 | Used Time: 6269.61 | Dis loss :  0.1766 | GP loss :  0.0343\n",
      "Used Step: 5888000 | Mean ep 100 return:  1.38 | Used Time: 6286.7 | Dis loss :  0.1254 | GP loss :  0.056\n",
      "Used Step: 5904000 | Mean ep 100 return:  1.04 | Used Time: 6304.05 | Dis loss :  0.1177 | GP loss :  0.0513\n",
      "Used Step: 5920000 | Mean ep 100 return:  1.01 | Used Time: 6321.21 | Dis loss :  0.1295 | GP loss :  0.0476\n",
      "Used Step: 5936000 | Mean ep 100 return:  1.16 | Used Time: 6338.38 | Dis loss :  0.2793 | GP loss :  0.1095\n",
      "Used Step: 5952000 | Mean ep 100 return:  1.18 | Used Time: 6355.55 | Dis loss :  0.1233 | GP loss :  0.0351\n",
      "Used Step: 5968000 | Mean ep 100 return:  1.14 | Used Time: 6372.8 | Dis loss :  0.1715 | GP loss :  0.0758\n",
      "Used Step: 5984000 | Mean ep 100 return:  1.46 | Used Time: 6389.9 | Dis loss :  0.1759 | GP loss :  0.0351\n",
      "Used Step: 6000000 | Mean ep 100 return:  1.58 | Used Time: 6406.89 | Dis loss :  0.1129 | GP loss :  0.054\n",
      "Used Step: 6016000 | Mean ep 100 return:  1.15 | Used Time: 6424.15 | Dis loss :  0.2428 | GP loss :  0.0424\n",
      "Used Step: 6032000 | Mean ep 100 return:  1.34 | Used Time: 6441.38 | Dis loss :  0.1644 | GP loss :  0.0335\n",
      "Used Step: 6048000 | Mean ep 100 return:  1.4 | Used Time: 6458.51 | Dis loss :  0.1134 | GP loss :  0.049\n",
      "Used Step: 6064000 | Mean ep 100 return:  1.63 | Used Time: 6475.48 | Dis loss :  0.2282 | GP loss :  0.0727\n",
      "Used Step: 6080000 | Mean ep 100 return:  1.63 | Used Time: 6492.47 | Dis loss :  0.1656 | GP loss :  0.0621\n",
      "Used Step: 6096000 | Mean ep 100 return:  1.36 | Used Time: 6509.63 | Dis loss :  0.1761 | GP loss :  0.0567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 6112000 | Mean ep 100 return:  1.29 | Used Time: 6526.87 | Dis loss :  0.1924 | GP loss :  0.0851\n",
      "Used Step: 6128000 | Mean ep 100 return:  1.25 | Used Time: 6543.92 | Dis loss :  0.2819 | GP loss :  0.0549\n",
      "Used Step: 6144000 | Mean ep 100 return:  1.27 | Used Time: 6560.88 | Dis loss :  0.3096 | GP loss :  0.0982\n",
      "Used Step: 6160000 | Mean ep 100 return:  1.52 | Used Time: 6577.94 | Dis loss :  0.2678 | GP loss :  0.0611\n",
      "Used Step: 6176000 | Mean ep 100 return:  1.5 | Used Time: 6594.84 | Dis loss :  0.1624 | GP loss :  0.0795\n",
      "Used Step: 6192000 | Mean ep 100 return:  1.02 | Used Time: 6611.94 | Dis loss :  0.1626 | GP loss :  0.0672\n",
      "Used Step: 6208000 | Mean ep 100 return:  1.32 | Used Time: 6628.88 | Dis loss :  0.1302 | GP loss :  0.0591\n",
      "Used Step: 6224000 | Mean ep 100 return:  1.09 | Used Time: 6646.13 | Dis loss :  0.1481 | GP loss :  0.0144\n",
      "Used Step: 6240000 | Mean ep 100 return:  0.85 | Used Time: 6663.4 | Dis loss :  0.2784 | GP loss :  0.0712\n",
      "Used Step: 6256000 | Mean ep 100 return:  0.99 | Used Time: 6680.61 | Dis loss :  0.5408 | GP loss :  0.1166\n",
      "Used Step: 6272000 | Mean ep 100 return:  1.07 | Used Time: 6697.95 | Dis loss :  0.1223 | GP loss :  0.0437\n",
      "Used Step: 6288000 | Mean ep 100 return:  0.7 | Used Time: 6715.61 | Dis loss :  0.2001 | GP loss :  0.0563\n",
      "Used Step: 6304000 | Mean ep 100 return:  0.83 | Used Time: 6733.0 | Dis loss :  0.2575 | GP loss :  0.0741\n",
      "Used Step: 6320000 | Mean ep 100 return:  1.13 | Used Time: 6750.03 | Dis loss :  0.1675 | GP loss :  0.0705\n",
      "Used Step: 6336000 | Mean ep 100 return:  1.02 | Used Time: 6767.2 | Dis loss :  0.1479 | GP loss :  0.0472\n",
      "Used Step: 6352000 | Mean ep 100 return:  1.31 | Used Time: 6784.52 | Dis loss :  0.1226 | GP loss :  0.0416\n",
      "Used Step: 6368000 | Mean ep 100 return:  1.18 | Used Time: 6801.89 | Dis loss :  0.1148 | GP loss :  0.0431\n",
      "Used Step: 6384000 | Mean ep 100 return:  1.01 | Used Time: 6819.16 | Dis loss :  0.1411 | GP loss :  0.0628\n",
      "Used Step: 6400000 | Mean ep 100 return:  1.04 | Used Time: 6836.51 | Dis loss :  0.1743 | GP loss :  0.0487\n",
      "Used Step: 6416000 | Mean ep 100 return:  0.96 | Used Time: 6853.89 | Dis loss :  0.1829 | GP loss :  0.0657\n",
      "Used Step: 6432000 | Mean ep 100 return:  0.98 | Used Time: 6871.12 | Dis loss :  0.1069 | GP loss :  0.0422\n",
      "Used Step: 6448000 | Mean ep 100 return:  0.71 | Used Time: 6888.31 | Dis loss :  0.1929 | GP loss :  0.044\n",
      "Used Step: 6464000 | Mean ep 100 return:  0.9 | Used Time: 6905.65 | Dis loss :  0.1781 | GP loss :  0.0619\n",
      "Used Step: 6480000 | Mean ep 100 return:  1.14 | Used Time: 6922.91 | Dis loss :  0.1795 | GP loss :  0.0691\n",
      "Used Step: 6496000 | Mean ep 100 return:  1.12 | Used Time: 6940.07 | Dis loss :  0.045 | GP loss :  0.0187\n",
      "Used Step: 6512000 | Mean ep 100 return:  0.95 | Used Time: 6957.18 | Dis loss :  0.18 | GP loss :  0.0739\n",
      "Used Step: 6528000 | Mean ep 100 return:  1.18 | Used Time: 6974.43 | Dis loss :  0.2194 | GP loss :  0.0575\n",
      "Used Step: 6544000 | Mean ep 100 return:  1.12 | Used Time: 6991.55 | Dis loss :  0.3311 | GP loss :  0.1066\n",
      "Used Step: 6560000 | Mean ep 100 return:  1.69 | Used Time: 7008.58 | Dis loss :  0.1308 | GP loss :  0.054\n",
      "Used Step: 6576000 | Mean ep 100 return:  2.02 | Used Time: 7025.6 | Dis loss :  0.1635 | GP loss :  0.0761\n",
      "Used Step: 6592000 | Mean ep 100 return:  1.29 | Used Time: 7042.77 | Dis loss :  0.2475 | GP loss :  0.1084\n",
      "Used Step: 6608000 | Mean ep 100 return:  0.89 | Used Time: 7060.13 | Dis loss :  0.1654 | GP loss :  0.06\n",
      "Used Step: 6624000 | Mean ep 100 return:  1.8 | Used Time: 7077.27 | Dis loss :  0.1311 | GP loss :  0.0377\n",
      "Used Step: 6640000 | Mean ep 100 return:  1.63 | Used Time: 7094.3 | Dis loss :  0.1273 | GP loss :  0.0477\n",
      "Used Step: 6656000 | Mean ep 100 return:  1.05 | Used Time: 7111.54 | Dis loss :  0.2224 | GP loss :  0.0815\n",
      "Used Step: 6672000 | Mean ep 100 return:  0.82 | Used Time: 7128.93 | Dis loss :  0.084 | GP loss :  0.0406\n",
      "Used Step: 6688000 | Mean ep 100 return:  1.16 | Used Time: 7146.11 | Dis loss :  0.2214 | GP loss :  0.0762\n",
      "Used Step: 6704000 | Mean ep 100 return:  1.68 | Used Time: 7163.21 | Dis loss :  0.174 | GP loss :  0.0542\n",
      "Used Step: 6720000 | Mean ep 100 return:  0.96 | Used Time: 7180.65 | Dis loss :  0.2586 | GP loss :  0.062\n",
      "Used Step: 6736000 | Mean ep 100 return:  1.16 | Used Time: 7197.86 | Dis loss :  0.1194 | GP loss :  0.0519\n",
      "Used Step: 6752000 | Mean ep 100 return:  0.99 | Used Time: 7215.11 | Dis loss :  0.1204 | GP loss :  0.0476\n",
      "Used Step: 6768000 | Mean ep 100 return:  0.97 | Used Time: 7232.49 | Dis loss :  0.1403 | GP loss :  0.061\n",
      "Used Step: 6784000 | Mean ep 100 return:  1.26 | Used Time: 7249.76 | Dis loss :  0.249 | GP loss :  0.0668\n",
      "Used Step: 6800000 | Mean ep 100 return:  1.67 | Used Time: 7266.82 | Dis loss :  0.3032 | GP loss :  0.0762\n",
      "Used Step: 6816000 | Mean ep 100 return:  1.9 | Used Time: 7283.92 | Dis loss :  0.1018 | GP loss :  0.0406\n",
      "Used Step: 6832000 | Mean ep 100 return:  1.42 | Used Time: 7301.22 | Dis loss :  0.1029 | GP loss :  0.0314\n",
      "Used Step: 6848000 | Mean ep 100 return:  0.99 | Used Time: 7318.56 | Dis loss :  0.1268 | GP loss :  0.0485\n",
      "Used Step: 6864000 | Mean ep 100 return:  1.27 | Used Time: 7335.94 | Dis loss :  0.2469 | GP loss :  0.091\n",
      "Used Step: 6880000 | Mean ep 100 return:  1.25 | Used Time: 7353.08 | Dis loss :  0.2531 | GP loss :  0.0648\n",
      "Used Step: 6896000 | Mean ep 100 return:  1.15 | Used Time: 7370.19 | Dis loss :  0.1976 | GP loss :  0.0827\n",
      "Used Step: 6912000 | Mean ep 100 return:  1.22 | Used Time: 7387.44 | Dis loss :  0.171 | GP loss :  0.0618\n",
      "Used Step: 6928000 | Mean ep 100 return:  1.54 | Used Time: 7404.62 | Dis loss :  0.2813 | GP loss :  0.0662\n",
      "Used Step: 6944000 | Mean ep 100 return:  0.99 | Used Time: 7421.8 | Dis loss :  0.1403 | GP loss :  0.0636\n",
      "Used Step: 6960000 | Mean ep 100 return:  1.1 | Used Time: 7438.89 | Dis loss :  0.1131 | GP loss :  0.0473\n",
      "Used Step: 6976000 | Mean ep 100 return:  1.23 | Used Time: 7456.32 | Dis loss :  0.2058 | GP loss :  0.0791\n",
      "Used Step: 6992000 | Mean ep 100 return:  0.94 | Used Time: 7473.79 | Dis loss :  0.1205 | GP loss :  0.057\n",
      "Used Step: 7008000 | Mean ep 100 return:  1.35 | Used Time: 7491.08 | Dis loss :  0.1336 | GP loss :  0.06\n",
      "Used Step: 7024000 | Mean ep 100 return:  1.27 | Used Time: 7508.31 | Dis loss :  0.3193 | GP loss :  0.0866\n",
      "Used Step: 7040000 | Mean ep 100 return:  0.85 | Used Time: 7525.71 | Dis loss :  0.0969 | GP loss :  0.0446\n",
      "Used Step: 7056000 | Mean ep 100 return:  1.52 | Used Time: 7542.71 | Dis loss :  0.2355 | GP loss :  0.09\n",
      "Used Step: 7072000 | Mean ep 100 return:  1.11 | Used Time: 7559.89 | Dis loss :  0.3423 | GP loss :  0.0842\n",
      "Used Step: 7088000 | Mean ep 100 return:  1.01 | Used Time: 7577.17 | Dis loss :  0.1119 | GP loss :  0.046\n",
      "Used Step: 7104000 | Mean ep 100 return:  1.22 | Used Time: 7594.53 | Dis loss :  0.1408 | GP loss :  0.0625\n",
      "Used Step: 7120000 | Mean ep 100 return:  1.06 | Used Time: 7611.89 | Dis loss :  0.1456 | GP loss :  0.0501\n",
      "Used Step: 7136000 | Mean ep 100 return:  1.49 | Used Time: 7628.98 | Dis loss :  0.1153 | GP loss :  0.0451\n",
      "Used Step: 7152000 | Mean ep 100 return:  1.04 | Used Time: 7646.12 | Dis loss :  0.191 | GP loss :  0.074\n",
      "Used Step: 7168000 | Mean ep 100 return:  1.26 | Used Time: 7663.32 | Dis loss :  0.0668 | GP loss :  0.0311\n",
      "Used Step: 7184000 | Mean ep 100 return:  1.17 | Used Time: 7680.51 | Dis loss :  0.1854 | GP loss :  0.0569\n",
      "Used Step: 7200000 | Mean ep 100 return:  1.4 | Used Time: 7697.78 | Dis loss :  0.1846 | GP loss :  0.1024\n",
      "Used Step: 7216000 | Mean ep 100 return:  1.24 | Used Time: 7715.04 | Dis loss :  0.1846 | GP loss :  0.0619\n",
      "Used Step: 7232000 | Mean ep 100 return:  1.11 | Used Time: 7732.3 | Dis loss :  0.1326 | GP loss :  0.0536\n",
      "Used Step: 7248000 | Mean ep 100 return:  1.15 | Used Time: 7749.41 | Dis loss :  0.1233 | GP loss :  0.0466\n",
      "Used Step: 7264000 | Mean ep 100 return:  0.87 | Used Time: 7766.77 | Dis loss :  0.2125 | GP loss :  0.0647\n",
      "Used Step: 7280000 | Mean ep 100 return:  0.82 | Used Time: 7784.18 | Dis loss :  0.285 | GP loss :  0.0708\n",
      "Used Step: 7296000 | Mean ep 100 return:  1.45 | Used Time: 7801.46 | Dis loss :  0.3061 | GP loss :  0.0775\n",
      "Used Step: 7312000 | Mean ep 100 return:  1.46 | Used Time: 7818.73 | Dis loss :  0.1307 | GP loss :  0.0384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 7328000 | Mean ep 100 return:  1.49 | Used Time: 7835.97 | Dis loss :  0.1573 | GP loss :  0.0666\n",
      "Used Step: 7344000 | Mean ep 100 return:  1.35 | Used Time: 7853.21 | Dis loss :  0.1244 | GP loss :  0.065\n",
      "Used Step: 7360000 | Mean ep 100 return:  1.0 | Used Time: 7870.47 | Dis loss :  0.1264 | GP loss :  0.0476\n",
      "Used Step: 7376000 | Mean ep 100 return:  1.35 | Used Time: 7887.44 | Dis loss :  0.1069 | GP loss :  0.0365\n",
      "Used Step: 7392000 | Mean ep 100 return:  1.65 | Used Time: 7904.67 | Dis loss :  0.0937 | GP loss :  0.0379\n",
      "Used Step: 7408000 | Mean ep 100 return:  1.2 | Used Time: 7921.93 | Dis loss :  0.1726 | GP loss :  0.0408\n",
      "Used Step: 7424000 | Mean ep 100 return:  1.33 | Used Time: 7938.96 | Dis loss :  0.1688 | GP loss :  0.0456\n",
      "Used Step: 7440000 | Mean ep 100 return:  1.72 | Used Time: 7956.03 | Dis loss :  0.2929 | GP loss :  0.0849\n",
      "Used Step: 7456000 | Mean ep 100 return:  1.16 | Used Time: 7973.24 | Dis loss :  0.1705 | GP loss :  0.0675\n",
      "Used Step: 7472000 | Mean ep 100 return:  1.15 | Used Time: 7990.42 | Dis loss :  0.142 | GP loss :  0.065\n",
      "Used Step: 7488000 | Mean ep 100 return:  1.0 | Used Time: 8007.62 | Dis loss :  0.1405 | GP loss :  0.0526\n",
      "Used Step: 7504000 | Mean ep 100 return:  0.82 | Used Time: 8025.07 | Dis loss :  0.1796 | GP loss :  0.0657\n",
      "Used Step: 7520000 | Mean ep 100 return:  1.33 | Used Time: 8042.26 | Dis loss :  0.0604 | GP loss :  0.0285\n",
      "Used Step: 7536000 | Mean ep 100 return:  1.2 | Used Time: 8059.36 | Dis loss :  0.166 | GP loss :  0.0685\n",
      "Used Step: 7552000 | Mean ep 100 return:  1.05 | Used Time: 8076.4 | Dis loss :  0.08 | GP loss :  0.0461\n",
      "Used Step: 7568000 | Mean ep 100 return:  1.35 | Used Time: 8093.72 | Dis loss :  0.36 | GP loss :  0.0844\n",
      "Used Step: 7584000 | Mean ep 100 return:  1.15 | Used Time: 8110.96 | Dis loss :  0.254 | GP loss :  0.084\n",
      "Used Step: 7600000 | Mean ep 100 return:  1.26 | Used Time: 8127.97 | Dis loss :  0.1652 | GP loss :  0.0612\n",
      "Used Step: 7616000 | Mean ep 100 return:  1.27 | Used Time: 8145.07 | Dis loss :  0.1826 | GP loss :  0.0653\n",
      "Used Step: 7632000 | Mean ep 100 return:  1.29 | Used Time: 8162.28 | Dis loss :  0.1452 | GP loss :  0.0538\n",
      "Used Step: 7648000 | Mean ep 100 return:  1.46 | Used Time: 8179.42 | Dis loss :  0.1206 | GP loss :  0.059\n",
      "Used Step: 7664000 | Mean ep 100 return:  1.71 | Used Time: 8196.36 | Dis loss :  0.2035 | GP loss :  0.0925\n",
      "Used Step: 7680000 | Mean ep 100 return:  1.59 | Used Time: 8213.28 | Dis loss :  0.1287 | GP loss :  0.0475\n",
      "Used Step: 7696000 | Mean ep 100 return:  1.01 | Used Time: 8230.59 | Dis loss :  0.1875 | GP loss :  0.0711\n",
      "Used Step: 7712000 | Mean ep 100 return:  0.96 | Used Time: 8247.92 | Dis loss :  0.1489 | GP loss :  0.0613\n",
      "Used Step: 7728000 | Mean ep 100 return:  1.29 | Used Time: 8264.99 | Dis loss :  0.1736 | GP loss :  0.0733\n",
      "Used Step: 7744000 | Mean ep 100 return:  1.45 | Used Time: 8282.05 | Dis loss :  0.1005 | GP loss :  0.0473\n",
      "Used Step: 7760000 | Mean ep 100 return:  1.87 | Used Time: 8299.26 | Dis loss :  0.1455 | GP loss :  0.0793\n",
      "Used Step: 7776000 | Mean ep 100 return:  1.07 | Used Time: 8316.61 | Dis loss :  0.2752 | GP loss :  0.0991\n",
      "Used Step: 7792000 | Mean ep 100 return:  1.62 | Used Time: 8333.71 | Dis loss :  0.1856 | GP loss :  0.0589\n",
      "Used Step: 7808000 | Mean ep 100 return:  1.78 | Used Time: 8350.85 | Dis loss :  0.1416 | GP loss :  0.0561\n",
      "Used Step: 7824000 | Mean ep 100 return:  1.44 | Used Time: 8368.24 | Dis loss :  0.4058 | GP loss :  0.0743\n",
      "Used Step: 7840000 | Mean ep 100 return:  1.15 | Used Time: 8385.8 | Dis loss :  0.0913 | GP loss :  0.041\n",
      "Used Step: 7856000 | Mean ep 100 return:  1.26 | Used Time: 8402.84 | Dis loss :  0.1854 | GP loss :  0.0665\n",
      "Used Step: 7872000 | Mean ep 100 return:  1.54 | Used Time: 8419.98 | Dis loss :  0.2118 | GP loss :  0.074\n",
      "Used Step: 7888000 | Mean ep 100 return:  1.86 | Used Time: 8437.18 | Dis loss :  0.1968 | GP loss :  0.0795\n",
      "Used Step: 7904000 | Mean ep 100 return:  1.56 | Used Time: 8454.34 | Dis loss :  0.1394 | GP loss :  0.0472\n",
      "Used Step: 7920000 | Mean ep 100 return:  1.13 | Used Time: 8471.53 | Dis loss :  0.2012 | GP loss :  0.0717\n",
      "Used Step: 7936000 | Mean ep 100 return:  1.02 | Used Time: 8488.74 | Dis loss :  0.1744 | GP loss :  0.0479\n",
      "Used Step: 7952000 | Mean ep 100 return:  0.77 | Used Time: 8506.31 | Dis loss :  0.2727 | GP loss :  0.0932\n",
      "Used Step: 7968000 | Mean ep 100 return:  0.54 | Used Time: 8524.01 | Dis loss :  0.2053 | GP loss :  0.043\n",
      "Used Step: 7984000 | Mean ep 100 return:  1.43 | Used Time: 8541.34 | Dis loss :  0.1775 | GP loss :  0.0768\n",
      "Used Step: 8000000 | Mean ep 100 return:  1.55 | Used Time: 8558.52 | Dis loss :  0.13 | GP loss :  0.0595\n",
      "Used Step: 8016000 | Mean ep 100 return:  1.09 | Used Time: 8575.94 | Dis loss :  0.1907 | GP loss :  0.0504\n",
      "Used Step: 8032000 | Mean ep 100 return:  1.1 | Used Time: 8593.17 | Dis loss :  0.2032 | GP loss :  0.0678\n",
      "Used Step: 8048000 | Mean ep 100 return:  0.89 | Used Time: 8610.41 | Dis loss :  0.1481 | GP loss :  0.0755\n",
      "Used Step: 8064000 | Mean ep 100 return:  1.25 | Used Time: 8627.49 | Dis loss :  0.2325 | GP loss :  0.0826\n",
      "Used Step: 8080000 | Mean ep 100 return:  1.56 | Used Time: 8644.8 | Dis loss :  0.2242 | GP loss :  0.0629\n",
      "Used Step: 8096000 | Mean ep 100 return:  1.23 | Used Time: 8662.08 | Dis loss :  0.1636 | GP loss :  0.055\n",
      "Used Step: 8112000 | Mean ep 100 return:  0.95 | Used Time: 8679.27 | Dis loss :  0.1479 | GP loss :  0.0554\n",
      "Used Step: 8128000 | Mean ep 100 return:  0.73 | Used Time: 8696.53 | Dis loss :  0.1383 | GP loss :  0.071\n",
      "Used Step: 8144000 | Mean ep 100 return:  0.8 | Used Time: 8713.91 | Dis loss :  0.2043 | GP loss :  0.0604\n",
      "Used Step: 8160000 | Mean ep 100 return:  1.22 | Used Time: 8730.94 | Dis loss :  0.1789 | GP loss :  0.0606\n",
      "Used Step: 8176000 | Mean ep 100 return:  0.96 | Used Time: 8748.15 | Dis loss :  0.1222 | GP loss :  0.0611\n",
      "Used Step: 8192000 | Mean ep 100 return:  1.37 | Used Time: 8765.42 | Dis loss :  0.1155 | GP loss :  0.0435\n",
      "Used Step: 8208000 | Mean ep 100 return:  1.22 | Used Time: 8782.78 | Dis loss :  0.0916 | GP loss :  0.04\n",
      "Used Step: 8224000 | Mean ep 100 return:  1.02 | Used Time: 8799.95 | Dis loss :  0.1503 | GP loss :  0.0722\n",
      "Used Step: 8240000 | Mean ep 100 return:  0.82 | Used Time: 8817.12 | Dis loss :  0.1234 | GP loss :  0.0494\n",
      "Used Step: 8256000 | Mean ep 100 return:  0.7 | Used Time: 8834.43 | Dis loss :  0.0934 | GP loss :  0.0412\n",
      "Used Step: 8272000 | Mean ep 100 return:  1.79 | Used Time: 8851.7 | Dis loss :  0.1479 | GP loss :  0.0661\n",
      "Used Step: 8288000 | Mean ep 100 return:  1.72 | Used Time: 8868.72 | Dis loss :  0.136 | GP loss :  0.077\n",
      "Used Step: 8304000 | Mean ep 100 return:  1.51 | Used Time: 8885.85 | Dis loss :  0.134 | GP loss :  0.0592\n",
      "Used Step: 8320000 | Mean ep 100 return:  1.21 | Used Time: 8903.23 | Dis loss :  0.1042 | GP loss :  0.0505\n",
      "Used Step: 8336000 | Mean ep 100 return:  1.12 | Used Time: 8920.58 | Dis loss :  0.2369 | GP loss :  0.0792\n",
      "Used Step: 8352000 | Mean ep 100 return:  1.2 | Used Time: 8937.82 | Dis loss :  0.1473 | GP loss :  0.0582\n",
      "Used Step: 8368000 | Mean ep 100 return:  1.8 | Used Time: 8954.82 | Dis loss :  0.2914 | GP loss :  0.0945\n",
      "Used Step: 8384000 | Mean ep 100 return:  1.28 | Used Time: 8971.96 | Dis loss :  0.1351 | GP loss :  0.0578\n",
      "Used Step: 8400000 | Mean ep 100 return:  1.85 | Used Time: 8989.14 | Dis loss :  0.1376 | GP loss :  0.0475\n",
      "Used Step: 8416000 | Mean ep 100 return:  1.58 | Used Time: 9006.13 | Dis loss :  0.2302 | GP loss :  0.092\n",
      "Used Step: 8432000 | Mean ep 100 return:  1.3 | Used Time: 9023.19 | Dis loss :  0.1848 | GP loss :  0.0545\n",
      "Used Step: 8448000 | Mean ep 100 return:  1.33 | Used Time: 9040.6 | Dis loss :  0.1602 | GP loss :  0.0396\n",
      "Used Step: 8464000 | Mean ep 100 return:  1.28 | Used Time: 9057.91 | Dis loss :  0.1936 | GP loss :  0.0582\n",
      "Used Step: 8480000 | Mean ep 100 return:  1.1 | Used Time: 9075.0 | Dis loss :  0.2112 | GP loss :  0.0734\n",
      "Used Step: 8496000 | Mean ep 100 return:  1.2 | Used Time: 9092.04 | Dis loss :  0.2684 | GP loss :  0.0661\n",
      "Used Step: 8512000 | Mean ep 100 return:  1.0 | Used Time: 9109.36 | Dis loss :  0.138 | GP loss :  0.0433\n",
      "Used Step: 8528000 | Mean ep 100 return:  1.12 | Used Time: 9126.55 | Dis loss :  0.3125 | GP loss :  0.0857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 8544000 | Mean ep 100 return:  1.4 | Used Time: 9143.69 | Dis loss :  0.1307 | GP loss :  0.0465\n",
      "Used Step: 8560000 | Mean ep 100 return:  1.36 | Used Time: 9161.37 | Dis loss :  0.276 | GP loss :  0.0549\n",
      "Used Step: 8576000 | Mean ep 100 return:  1.44 | Used Time: 9178.85 | Dis loss :  0.1233 | GP loss :  0.0505\n",
      "Used Step: 8592000 | Mean ep 100 return:  1.72 | Used Time: 9195.93 | Dis loss :  0.1781 | GP loss :  0.0733\n",
      "Used Step: 8608000 | Mean ep 100 return:  1.42 | Used Time: 9213.04 | Dis loss :  0.1258 | GP loss :  0.0548\n",
      "Used Step: 8624000 | Mean ep 100 return:  1.7 | Used Time: 9230.24 | Dis loss :  0.4003 | GP loss :  0.114\n",
      "Used Step: 8640000 | Mean ep 100 return:  1.05 | Used Time: 9247.61 | Dis loss :  0.2909 | GP loss :  0.0933\n",
      "Used Step: 8656000 | Mean ep 100 return:  1.28 | Used Time: 9264.94 | Dis loss :  0.0858 | GP loss :  0.032\n",
      "Used Step: 8672000 | Mean ep 100 return:  1.1 | Used Time: 9282.0 | Dis loss :  0.1971 | GP loss :  0.0606\n",
      "Used Step: 8688000 | Mean ep 100 return:  1.4 | Used Time: 9299.04 | Dis loss :  0.2402 | GP loss :  0.079\n",
      "Used Step: 8704000 | Mean ep 100 return:  1.86 | Used Time: 9316.11 | Dis loss :  0.133 | GP loss :  0.0482\n",
      "Used Step: 8720000 | Mean ep 100 return:  1.63 | Used Time: 9333.12 | Dis loss :  0.3014 | GP loss :  0.0697\n",
      "Used Step: 8736000 | Mean ep 100 return:  1.32 | Used Time: 9350.11 | Dis loss :  0.2247 | GP loss :  0.0677\n",
      "Used Step: 8752000 | Mean ep 100 return:  1.01 | Used Time: 9367.4 | Dis loss :  0.1969 | GP loss :  0.0716\n",
      "Used Step: 8768000 | Mean ep 100 return:  1.09 | Used Time: 9384.69 | Dis loss :  0.1583 | GP loss :  0.061\n",
      "Used Step: 8784000 | Mean ep 100 return:  1.36 | Used Time: 9401.92 | Dis loss :  0.2373 | GP loss :  0.0685\n",
      "Used Step: 8800000 | Mean ep 100 return:  1.54 | Used Time: 9418.95 | Dis loss :  0.2345 | GP loss :  0.0852\n",
      "Used Step: 8816000 | Mean ep 100 return:  1.25 | Used Time: 9436.18 | Dis loss :  0.2318 | GP loss :  0.0772\n",
      "Used Step: 8832000 | Mean ep 100 return:  1.72 | Used Time: 9453.22 | Dis loss :  0.2143 | GP loss :  0.0591\n",
      "Used Step: 8848000 | Mean ep 100 return:  1.42 | Used Time: 9470.44 | Dis loss :  0.1119 | GP loss :  0.0459\n",
      "Used Step: 8864000 | Mean ep 100 return:  1.36 | Used Time: 9487.41 | Dis loss :  0.1902 | GP loss :  0.0492\n",
      "Used Step: 8880000 | Mean ep 100 return:  0.89 | Used Time: 9504.81 | Dis loss :  0.207 | GP loss :  0.071\n",
      "Used Step: 8896000 | Mean ep 100 return:  1.83 | Used Time: 9521.91 | Dis loss :  0.2289 | GP loss :  0.0773\n",
      "Used Step: 8912000 | Mean ep 100 return:  2.21 | Used Time: 9538.85 | Dis loss :  0.2327 | GP loss :  0.0963\n",
      "Used Step: 8928000 | Mean ep 100 return:  1.41 | Used Time: 9555.95 | Dis loss :  0.1362 | GP loss :  0.0395\n",
      "Used Step: 8944000 | Mean ep 100 return:  1.18 | Used Time: 9573.31 | Dis loss :  0.1701 | GP loss :  0.0292\n",
      "Used Step: 8960000 | Mean ep 100 return:  1.32 | Used Time: 9590.6 | Dis loss :  0.195 | GP loss :  0.0844\n",
      "Used Step: 8976000 | Mean ep 100 return:  1.24 | Used Time: 9607.7 | Dis loss :  0.2529 | GP loss :  0.0963\n",
      "Used Step: 8992000 | Mean ep 100 return:  1.24 | Used Time: 9624.83 | Dis loss :  0.1266 | GP loss :  0.0458\n",
      "Used Step: 9008000 | Mean ep 100 return:  1.55 | Used Time: 9642.13 | Dis loss :  0.0799 | GP loss :  0.0373\n",
      "Used Step: 9024000 | Mean ep 100 return:  1.34 | Used Time: 9659.33 | Dis loss :  0.1846 | GP loss :  0.0804\n",
      "Used Step: 9040000 | Mean ep 100 return:  1.64 | Used Time: 9676.63 | Dis loss :  0.0874 | GP loss :  0.0319\n",
      "Used Step: 9056000 | Mean ep 100 return:  1.3 | Used Time: 9693.76 | Dis loss :  0.1567 | GP loss :  0.076\n",
      "Used Step: 9072000 | Mean ep 100 return:  1.23 | Used Time: 9710.88 | Dis loss :  0.2166 | GP loss :  0.0917\n",
      "Used Step: 9088000 | Mean ep 100 return:  0.99 | Used Time: 9728.11 | Dis loss :  0.0909 | GP loss :  0.047\n",
      "Used Step: 9104000 | Mean ep 100 return:  1.39 | Used Time: 9745.12 | Dis loss :  0.0871 | GP loss :  0.0278\n",
      "Used Step: 9120000 | Mean ep 100 return:  1.39 | Used Time: 9762.24 | Dis loss :  0.193 | GP loss :  0.0686\n",
      "Used Step: 9136000 | Mean ep 100 return:  0.89 | Used Time: 9779.59 | Dis loss :  0.0696 | GP loss :  0.0235\n",
      "Used Step: 9152000 | Mean ep 100 return:  0.85 | Used Time: 9797.01 | Dis loss :  0.1139 | GP loss :  0.0356\n",
      "Used Step: 9168000 | Mean ep 100 return:  1.37 | Used Time: 9814.02 | Dis loss :  0.1612 | GP loss :  0.0679\n",
      "Used Step: 9184000 | Mean ep 100 return:  1.31 | Used Time: 9831.06 | Dis loss :  0.1404 | GP loss :  0.062\n",
      "Used Step: 9200000 | Mean ep 100 return:  1.21 | Used Time: 9848.32 | Dis loss :  0.3701 | GP loss :  0.0919\n",
      "Used Step: 9216000 | Mean ep 100 return:  1.01 | Used Time: 9865.63 | Dis loss :  0.2279 | GP loss :  0.0569\n",
      "Used Step: 9232000 | Mean ep 100 return:  0.87 | Used Time: 9882.89 | Dis loss :  0.4289 | GP loss :  0.1018\n",
      "Used Step: 9248000 | Mean ep 100 return:  0.94 | Used Time: 9900.19 | Dis loss :  0.1456 | GP loss :  0.0398\n",
      "Used Step: 9264000 | Mean ep 100 return:  1.28 | Used Time: 9917.36 | Dis loss :  0.1099 | GP loss :  0.0438\n",
      "Used Step: 9280000 | Mean ep 100 return:  0.63 | Used Time: 9934.86 | Dis loss :  0.1331 | GP loss :  0.0571\n",
      "Used Step: 9296000 | Mean ep 100 return:  0.78 | Used Time: 9952.41 | Dis loss :  0.2176 | GP loss :  0.0718\n",
      "Used Step: 9312000 | Mean ep 100 return:  2.34 | Used Time: 9969.38 | Dis loss :  0.2356 | GP loss :  0.0738\n",
      "Used Step: 9328000 | Mean ep 100 return:  1.34 | Used Time: 9986.57 | Dis loss :  0.2466 | GP loss :  0.053\n",
      "Used Step: 9344000 | Mean ep 100 return:  1.53 | Used Time: 10003.9 | Dis loss :  0.0883 | GP loss :  0.0309\n",
      "Used Step: 9360000 | Mean ep 100 return:  1.15 | Used Time: 10021.16 | Dis loss :  0.1116 | GP loss :  0.0389\n",
      "Used Step: 9376000 | Mean ep 100 return:  1.29 | Used Time: 10038.35 | Dis loss :  0.1899 | GP loss :  0.0892\n",
      "Used Step: 9392000 | Mean ep 100 return:  1.08 | Used Time: 10055.52 | Dis loss :  0.1363 | GP loss :  0.0379\n",
      "Used Step: 9408000 | Mean ep 100 return:  1.04 | Used Time: 10072.78 | Dis loss :  0.1209 | GP loss :  0.0483\n",
      "Used Step: 9424000 | Mean ep 100 return:  1.39 | Used Time: 10089.8 | Dis loss :  0.1882 | GP loss :  0.0759\n",
      "Used Step: 9440000 | Mean ep 100 return:  1.18 | Used Time: 10106.96 | Dis loss :  0.1385 | GP loss :  0.0589\n",
      "Used Step: 9456000 | Mean ep 100 return:  1.16 | Used Time: 10124.2 | Dis loss :  0.1348 | GP loss :  0.0512\n",
      "Used Step: 9472000 | Mean ep 100 return:  1.47 | Used Time: 10141.49 | Dis loss :  0.1652 | GP loss :  0.0449\n",
      "Used Step: 9488000 | Mean ep 100 return:  2.23 | Used Time: 10158.56 | Dis loss :  0.065 | GP loss :  0.0306\n",
      "Used Step: 9504000 | Mean ep 100 return:  1.13 | Used Time: 10175.69 | Dis loss :  0.1816 | GP loss :  0.0778\n",
      "Used Step: 9520000 | Mean ep 100 return:  1.05 | Used Time: 10192.98 | Dis loss :  0.2543 | GP loss :  0.0882\n",
      "Used Step: 9536000 | Mean ep 100 return:  0.9 | Used Time: 10210.36 | Dis loss :  0.142 | GP loss :  0.0823\n",
      "Used Step: 9552000 | Mean ep 100 return:  0.97 | Used Time: 10227.5 | Dis loss :  0.3183 | GP loss :  0.077\n",
      "Used Step: 9568000 | Mean ep 100 return:  1.22 | Used Time: 10244.51 | Dis loss :  0.1197 | GP loss :  0.0554\n",
      "Used Step: 9584000 | Mean ep 100 return:  1.39 | Used Time: 10261.73 | Dis loss :  0.0886 | GP loss :  0.0428\n",
      "Used Step: 9600000 | Mean ep 100 return:  1.15 | Used Time: 10279.2 | Dis loss :  0.1629 | GP loss :  0.0338\n",
      "Used Step: 9616000 | Mean ep 100 return:  1.14 | Used Time: 10296.35 | Dis loss :  0.2532 | GP loss :  0.0925\n",
      "Used Step: 9632000 | Mean ep 100 return:  1.11 | Used Time: 10313.36 | Dis loss :  0.1371 | GP loss :  0.0612\n",
      "Used Step: 9648000 | Mean ep 100 return:  1.77 | Used Time: 10330.4 | Dis loss :  0.1326 | GP loss :  0.0481\n",
      "Used Step: 9664000 | Mean ep 100 return:  1.46 | Used Time: 10347.44 | Dis loss :  0.1204 | GP loss :  0.0522\n",
      "Used Step: 9680000 | Mean ep 100 return:  1.32 | Used Time: 10364.56 | Dis loss :  0.1977 | GP loss :  0.0556\n",
      "Used Step: 9696000 | Mean ep 100 return:  1.7 | Used Time: 10381.68 | Dis loss :  0.2359 | GP loss :  0.0765\n",
      "Used Step: 9712000 | Mean ep 100 return:  1.48 | Used Time: 10398.96 | Dis loss :  0.0994 | GP loss :  0.0479\n",
      "Used Step: 9728000 | Mean ep 100 return:  0.97 | Used Time: 10416.09 | Dis loss :  0.1578 | GP loss :  0.0703\n",
      "Used Step: 9744000 | Mean ep 100 return:  1.23 | Used Time: 10433.23 | Dis loss :  0.1624 | GP loss :  0.0658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 9760000 | Mean ep 100 return:  1.29 | Used Time: 10450.43 | Dis loss :  0.1258 | GP loss :  0.0468\n",
      "Used Step: 9776000 | Mean ep 100 return:  1.6 | Used Time: 10467.61 | Dis loss :  0.2374 | GP loss :  0.0686\n",
      "Used Step: 9792000 | Mean ep 100 return:  1.19 | Used Time: 10484.64 | Dis loss :  0.2087 | GP loss :  0.0535\n",
      "Used Step: 9808000 | Mean ep 100 return:  1.44 | Used Time: 10501.6 | Dis loss :  0.0645 | GP loss :  0.0258\n",
      "Used Step: 9824000 | Mean ep 100 return:  2.52 | Used Time: 10518.6 | Dis loss :  0.1299 | GP loss :  0.0466\n",
      "Used Step: 9840000 | Mean ep 100 return:  1.18 | Used Time: 10535.81 | Dis loss :  0.1435 | GP loss :  0.0757\n",
      "Used Step: 9856000 | Mean ep 100 return:  1.31 | Used Time: 10552.89 | Dis loss :  0.1797 | GP loss :  0.0604\n",
      "Used Step: 9872000 | Mean ep 100 return:  1.41 | Used Time: 10569.98 | Dis loss :  0.1348 | GP loss :  0.0637\n",
      "Used Step: 9888000 | Mean ep 100 return:  1.45 | Used Time: 10587.08 | Dis loss :  0.1956 | GP loss :  0.0489\n",
      "Used Step: 9904000 | Mean ep 100 return:  0.9 | Used Time: 10604.29 | Dis loss :  0.1376 | GP loss :  0.0672\n",
      "Used Step: 9920000 | Mean ep 100 return:  1.17 | Used Time: 10621.32 | Dis loss :  0.2935 | GP loss :  0.0731\n",
      "Used Step: 9936000 | Mean ep 100 return:  1.93 | Used Time: 10638.35 | Dis loss :  0.1598 | GP loss :  0.0671\n",
      "Used Step: 9952000 | Mean ep 100 return:  1.44 | Used Time: 10655.38 | Dis loss :  0.1864 | GP loss :  0.0513\n",
      "Used Step: 9968000 | Mean ep 100 return:  0.97 | Used Time: 10672.38 | Dis loss :  0.1547 | GP loss :  0.0627\n",
      "Used Step: 9984000 | Mean ep 100 return:  1.3 | Used Time: 10689.45 | Dis loss :  0.0994 | GP loss :  0.0392\n",
      "Used Step: 10000000 | Mean ep 100 return:  1.33 | Used Time: 10706.58 | Dis loss :  0.1103 | GP loss :  0.0384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sac = SAC()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    sac.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "# env reset\n",
    "s = np.array(env.reset())\n",
    "\n",
    "for step in tqdm_notebook(range(1, N_STEP//N_ENVS + 1)):\n",
    "    \n",
    "    a = sac.choose_action(s)\n",
    "    \n",
    "    # take action and get next state\n",
    "    s_, r, done, infos = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    \n",
    "    # log arrange\n",
    "    for info in infos:\n",
    "        maybeepinfo = info.get('episode')\n",
    "        if maybeepinfo: epinfobuf.append(maybeepinfo)\n",
    "            \n",
    "    # store transition\n",
    "    for i in range(len(s_)):\n",
    "        sac.store_transition(s[i],a[i],None,s_[i], done[i])\n",
    "    \n",
    "    if (step >= LEARN_START) and (step % LEARN_FREQ == 0):\n",
    "        d_loss, gp_loss = sac.learn_dis()\n",
    "        sac.learn()\n",
    "        \n",
    "    s = s_\n",
    "            \n",
    "    if step % LOG_FREQ == 0:\n",
    "        # print log and save\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        result.append(mean_100_ep_return)\n",
    "        # print epi log\n",
    "        print('Used Step:',sac.memory_counter,\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '| Used Time:',time_interval,\n",
    "             '| Dis loss : ',d_loss,\n",
    "             '| GP loss : ', gp_loss)\n",
    "        # save model\n",
    "        if SAVE:\n",
    "            sac.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(result)), result)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./gail_breakout_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward : 0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAAD+CAYAAACgNTAxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA/hJREFUeJzt3bFt1GAYgGGMUiMmoKJghIgBohQsQyZgghsDMQBFlIISZRhEgRBFipg6ke5OOb8X25fnaR07f4o33/lX7AzjOL4Cpns99wLgVIgJImKCiJggIiaIiAkiYoKImCAiJoiczb2AbYZh8KcZLM44jsO2YyYTRMQEETFBZLH3TEu02WyefM7V1dWkazw+v7rGVI/XsO/nPMYanrqmYzOZIGIyTXCMqTHH9KNhMkHEZCJhGppMkDGZOMi+nbKXOKlMJoiYTBMUv32Xcg2mM5kgIiaIDEt9CaVHMFgij2DAM1jsBoSbatbGZIKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCCy2Ecw9nnu90jzMkx59MdkgoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigshq3+h6e3k59xI4QT8nnGsyQURMEBETRMQEETFBZLW7effv/8y9BHjAZIKImCAiJoiICSJigoiYILLarfHfb/7NvQR4wGSCiJggIiaIiAkiYoLIenfzPtzNvQRO0a/DTzWZICImiIgJImKCiJggstrdvK/37+ZeAifoYsK5JhNExAQRMUFETBARE0RWu5t39+3L3EvgFF0c/k9lTCaIiAkiYoKImCAiJoiICSKr3Rr/cX0+9xI4QZ8uNgefazJBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQERNExAQRMUFETBARE0TEBBExQURMEBETRMQEETFBREwQOZt7Adt8f/t35/Hby8vJ3+P8+nryNTgtH29udn/B589bD5lMEBETRMQEkcXeM+3jfoelMZkgstrJBMew7xPPuOPYMI67Ds9nGIZlLowXbRzHYdsxH/MgIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaILPYRDFgbkwkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoKImCAiJoiICSJigoiYICImiIgJImKCiJggIiaIiAkiYoLIfxqjYB7TDjXlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = wrap(gym.make('BreakoutNoFrameskip-v4'))\n",
    "s = np.array(env.reset())\n",
    "total_reward = 0\n",
    "frames = []\n",
    "done_stack = 0\n",
    "\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a = sac.choose_action(np.expand_dims(s,axis=0))\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        done_stack += 1\n",
    "        if done_stack == 5:\n",
    "            break\n",
    "    s = s_\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./gail_breakout_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

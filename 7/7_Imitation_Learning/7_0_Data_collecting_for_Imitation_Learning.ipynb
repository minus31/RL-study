{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import deque\n",
    "from replay_memory import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap, wrap_cover, SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_ACTIONS :  4\n",
      "N_STATES :  (4, 84, 84)\n",
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''SAC Settings'''\n",
    "# coefficient of entropy regularization\n",
    "ENT_COEF = 1e-2\n",
    "# experience replay memory size\n",
    "MEMORY_CAPACITY = 10**6\n",
    "# learn start\n",
    "LEARN_START = int(1e+3)\n",
    "# learn frequency\n",
    "LEARN_FREQ = 1\n",
    "\n",
    "'''Environment Settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "# number of environments for SAC\n",
    "N_ENVS = 1\n",
    "# define gym \n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "# check gym setting\n",
    "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ',N_ACTIONS) #  4\n",
    "N_STATES = env.observation_space.shape;print('N_STATES : ',N_STATES) # (4, 84, 84)\n",
    "# Total simulation step\n",
    "N_STEP = 10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# clip gradient\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# log frequency\n",
    "LOG_FREQ = int(1e+3)\n",
    "# check save/load\n",
    "SAVE = False\n",
    "LOAD = True\n",
    "# paths for predction net, target net, result log\n",
    "ACTOR_PATH = './data/model/actor_net.pkl'\n",
    "CRITIC_PATH = './data/model/critic_net.pkl'\n",
    "ACTION_CRITIC_PATH = './data/model/action_critic_net.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.actor = nn.Linear(256, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_log_prob = F.log_softmax(self.actor(x), dim=1)\n",
    "\n",
    "        return action_log_prob\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),ACTOR_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(ACTOR_PATH))\n",
    "        \n",
    "class CriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),CRITIC_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(CRITIC_PATH))\n",
    "        \n",
    "class ActionCriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionCriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 256)\n",
    "        # actor\n",
    "        self.action_critic = nn.Linear(256, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_value = self.action_critic(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),ACTION_CRITIC_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(ACTION_CRITIC_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self):\n",
    "        self.actor_net = ActorConvNet()\n",
    "        self.critic_net = CriticConvNet()\n",
    "        self.critic_target = CriticConvNet()\n",
    "        self.action_critic_net = ActionCriticConvNet()\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.actor_net = self.actor_net.cuda(device=0)\n",
    "            self.critic_net = self.critic_net.cuda(device=0)\n",
    "            # critic target network for stability\n",
    "            self.critic_target = self.critic_net.cuda(device=0)\n",
    "            self.action_critic_net = self.action_critic_net.cuda(device=0)\n",
    "        \n",
    "        # sync net and target\n",
    "        self.critic_target.load_state_dict(self.critic_net.state_dict())\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # Create the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "            \n",
    "        # define optimizer\n",
    "        self.actor_opt = torch.optim.Adam(self.actor_net.parameters(), lr=LR)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic_net.parameters(), lr=LR)\n",
    "        self.action_critic_opt = torch.optim.Adam(self.action_critic_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        \n",
    "        self.actor_net.save(ACTOR_PATH)\n",
    "        self.critic_net.save(CRITIC_PATH)\n",
    "        self.action_critic_net.save(ACTION_CRITIC_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=0)\n",
    "            self.critic_net.cuda(device=0)\n",
    "            self.action_critic_net.cuda(device=0)\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        \n",
    "        self.actor_net.load(ACTOR_PATH)\n",
    "        self.critic_net.load(CRITIC_PATH)\n",
    "        self.action_critic_net.load(ACTION_CRITIC_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=0)\n",
    "            self.critic_net.cuda(device=0)\n",
    "            self.action_critic_net.cuda(device=0)\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda(device=0)\n",
    "        # get action log probs and state values\n",
    "        action_log_prob = self.actor_net(x)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1).data.cpu().numpy()\n",
    "        # sample actions\n",
    "        action = np.array([np.random.choice(N_ACTIONS,p=action_prob[i]) for i in range(len(action_prob))])\n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "\n",
    "    def learn(self):\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        # data sample from experience replay\n",
    "        b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "        b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        b_weights, b_idxes = np.ones_like(b_reward_memory), None\n",
    "            \n",
    "        b_s = torch.FloatTensor(np.array(b_state_memory))\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_r = torch.FloatTensor(b_reward_memory)\n",
    "        b_s_ = torch.FloatTensor(np.array(b_next_state_memory))\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(device=0), b_a.cuda(device=0), b_r.cuda(device=0), b_s_.cuda(device=0), b_d.cuda(device=0)\n",
    "            \n",
    "        # forward calc\n",
    "        action_log_prob = self.actor_net(b_s)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1)\n",
    "        action_log_prob = F.log_softmax(action_log_prob, dim=1)\n",
    "        cur_value = self.critic_net(b_s).squeeze(1)\n",
    "        next_value = self.critic_target(b_s_)\n",
    "        action_value = self.action_critic_net(b_s)\n",
    "\n",
    "        # critic loss. eq (5) in SAC paper\n",
    "        value_target = (action_value - ENT_COEF * action_log_prob).gather(1, b_a.unsqueeze(1)).squeeze(1)\n",
    "        critic_loss = 0.5 * F.smooth_l1_loss(cur_value, value_target.detach())\n",
    "\n",
    "        # action critic loss. eq (7), (8) in SAC paper\n",
    "        action_value_target = b_r + GAMMA * (1-b_d) * next_value.squeeze(1)\n",
    "        action_critic_loss = 0.5 * F.smooth_l1_loss(action_value.gather(1, \n",
    "            b_a.unsqueeze(1)).squeeze(1), action_value_target.detach())\n",
    "\n",
    "        # actor loss. eq (10) in SAC paper\n",
    "        actor_loss = torch.mean(action_prob*(action_log_prob \\\n",
    "            - F.log_softmax(action_value.detach()/ENT_COEF, dim=1)))\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.actor_opt.step()\n",
    "        \n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        self.action_critic_opt.zero_grad()\n",
    "        action_critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.action_critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.action_critic_opt.step()\n",
    "        \n",
    "        self.update_target(self.critic_target, self.critic_net, 1e-3)\n",
    "        \n",
    "        return round(float(actor_loss), 4), round(float(critic_loss), 4), round(float(action_critic_loss), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 모으기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load complete!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2078cfc8e5044e69f6582d97ecfe153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sungyubkim/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 1000 | Mean ep 100 return:  nan | Used Time: 1.87\n",
      "Used Step: 2000 | Mean ep 100 return:  292.0 | Used Time: 3.64\n",
      "Used Step: 3000 | Mean ep 100 return:  292.0 | Used Time: 5.43\n",
      "Used Step: 4000 | Mean ep 100 return:  305.0 | Used Time: 7.23\n",
      "Used Step: 5000 | Mean ep 100 return:  305.0 | Used Time: 9.12\n",
      "Used Step: 6000 | Mean ep 100 return:  305.0 | Used Time: 11.04\n",
      "Used Step: 7000 | Mean ep 100 return:  347.0 | Used Time: 12.8\n",
      "Used Step: 8000 | Mean ep 100 return:  347.0 | Used Time: 14.56\n",
      "Used Step: 9000 | Mean ep 100 return:  347.0 | Used Time: 16.29\n",
      "Used Step: 10000 | Mean ep 100 return:  347.0 | Used Time: 18.02\n",
      "Used Step: 11000 | Mean ep 100 return:  347.0 | Used Time: 19.75\n",
      "Used Step: 12000 | Mean ep 100 return:  347.0 | Used Time: 21.47\n",
      "Used Step: 13000 | Mean ep 100 return:  347.0 | Used Time: 23.2\n",
      "Used Step: 14000 | Mean ep 100 return:  347.0 | Used Time: 24.91\n",
      "Used Step: 15000 | Mean ep 100 return:  367.25 | Used Time: 26.64\n",
      "Used Step: 16000 | Mean ep 100 return:  367.25 | Used Time: 28.35\n",
      "Used Step: 17000 | Mean ep 100 return:  374.6 | Used Time: 30.11\n",
      "Used Step: 18000 | Mean ep 100 return:  374.6 | Used Time: 31.86\n",
      "Used Step: 19000 | Mean ep 100 return:  376.83 | Used Time: 33.61\n",
      "Used Step: 20000 | Mean ep 100 return:  376.83 | Used Time: 35.38\n",
      "Used Step: 21000 | Mean ep 100 return:  376.83 | Used Time: 37.1\n",
      "Used Step: 22000 | Mean ep 100 return:  376.83 | Used Time: 38.8\n",
      "Used Step: 23000 | Mean ep 100 return:  376.83 | Used Time: 40.5\n",
      "Used Step: 24000 | Mean ep 100 return:  376.83 | Used Time: 42.2\n",
      "Used Step: 25000 | Mean ep 100 return:  376.83 | Used Time: 43.89\n",
      "Used Step: 26000 | Mean ep 100 return:  384.14 | Used Time: 45.62\n",
      "Used Step: 27000 | Mean ep 100 return:  384.14 | Used Time: 47.36\n",
      "Used Step: 28000 | Mean ep 100 return:  389.62 | Used Time: 49.07\n",
      "Used Step: 29000 | Mean ep 100 return:  389.62 | Used Time: 50.83\n",
      "Used Step: 30000 | Mean ep 100 return:  354.56 | Used Time: 52.64\n",
      "Used Step: 31000 | Mean ep 100 return:  354.56 | Used Time: 54.39\n",
      "Used Step: 32000 | Mean ep 100 return:  359.1 | Used Time: 56.09\n",
      "Used Step: 33000 | Mean ep 100 return:  359.1 | Used Time: 57.82\n",
      "Used Step: 34000 | Mean ep 100 return:  330.27 | Used Time: 59.63\n",
      "Used Step: 35000 | Mean ep 100 return:  309.25 | Used Time: 61.46\n",
      "Used Step: 36000 | Mean ep 100 return:  309.25 | Used Time: 63.21\n",
      "Used Step: 37000 | Mean ep 100 return:  309.25 | Used Time: 64.95\n",
      "Used Step: 38000 | Mean ep 100 return:  317.92 | Used Time: 66.7\n",
      "Used Step: 39000 | Mean ep 100 return:  317.92 | Used Time: 68.47\n",
      "Used Step: 40000 | Mean ep 100 return:  325.43 | Used Time: 70.19\n",
      "Used Step: 41000 | Mean ep 100 return:  325.43 | Used Time: 71.95\n",
      "Used Step: 42000 | Mean ep 100 return:  330.47 | Used Time: 73.71\n",
      "Used Step: 43000 | Mean ep 100 return:  330.47 | Used Time: 75.46\n",
      "Used Step: 44000 | Mean ep 100 return:  330.47 | Used Time: 77.2\n",
      "Used Step: 45000 | Mean ep 100 return:  335.62 | Used Time: 78.89\n",
      "Used Step: 46000 | Mean ep 100 return:  317.82 | Used Time: 80.63\n",
      "Used Step: 47000 | Mean ep 100 return:  317.82 | Used Time: 82.39\n",
      "Used Step: 48000 | Mean ep 100 return:  320.06 | Used Time: 84.16\n",
      "Used Step: 49000 | Mean ep 100 return:  320.06 | Used Time: 85.9\n",
      "Used Step: 50000 | Mean ep 100 return:  320.06 | Used Time: 87.6\n",
      "Used Step: 51000 | Mean ep 100 return:  326.58 | Used Time: 89.33\n",
      "Used Step: 52000 | Mean ep 100 return:  326.58 | Used Time: 91.07\n",
      "Used Step: 53000 | Mean ep 100 return:  326.58 | Used Time: 92.79\n",
      "Used Step: 54000 | Mean ep 100 return:  326.58 | Used Time: 94.49\n",
      "Used Step: 55000 | Mean ep 100 return:  326.58 | Used Time: 96.25\n",
      "Used Step: 56000 | Mean ep 100 return:  350.5 | Used Time: 98.01\n",
      "Used Step: 57000 | Mean ep 100 return:  350.5 | Used Time: 99.76\n",
      "Used Step: 58000 | Mean ep 100 return:  350.5 | Used Time: 101.48\n",
      "Used Step: 59000 | Mean ep 100 return:  350.5 | Used Time: 103.2\n",
      "Used Step: 60000 | Mean ep 100 return:  350.5 | Used Time: 104.91\n",
      "Used Step: 61000 | Mean ep 100 return:  353.71 | Used Time: 106.63\n",
      "Used Step: 62000 | Mean ep 100 return:  353.71 | Used Time: 108.4\n",
      "Used Step: 63000 | Mean ep 100 return:  353.71 | Used Time: 110.14\n",
      "Used Step: 64000 | Mean ep 100 return:  355.68 | Used Time: 111.92\n",
      "Used Step: 65000 | Mean ep 100 return:  355.68 | Used Time: 113.66\n",
      "Used Step: 66000 | Mean ep 100 return:  357.87 | Used Time: 115.42\n",
      "Used Step: 67000 | Mean ep 100 return:  357.87 | Used Time: 117.16\n",
      "Used Step: 68000 | Mean ep 100 return:  357.87 | Used Time: 118.87\n",
      "Used Step: 69000 | Mean ep 100 return:  357.87 | Used Time: 120.57\n",
      "Used Step: 70000 | Mean ep 100 return:  357.87 | Used Time: 122.27\n",
      "Used Step: 71000 | Mean ep 100 return:  357.87 | Used Time: 123.96\n",
      "Used Step: 72000 | Mean ep 100 return:  357.87 | Used Time: 125.65\n",
      "Used Step: 73000 | Mean ep 100 return:  360.62 | Used Time: 127.41\n",
      "Used Step: 74000 | Mean ep 100 return:  360.62 | Used Time: 129.16\n",
      "Used Step: 75000 | Mean ep 100 return:  361.76 | Used Time: 130.91\n",
      "Used Step: 76000 | Mean ep 100 return:  363.31 | Used Time: 132.65\n",
      "Used Step: 77000 | Mean ep 100 return:  363.31 | Used Time: 134.41\n",
      "Used Step: 78000 | Mean ep 100 return:  363.31 | Used Time: 136.16\n",
      "Used Step: 79000 | Mean ep 100 return:  364.11 | Used Time: 137.91\n",
      "Used Step: 80000 | Mean ep 100 return:  364.11 | Used Time: 139.65\n",
      "Used Step: 81000 | Mean ep 100 return:  365.57 | Used Time: 141.37\n",
      "Used Step: 82000 | Mean ep 100 return:  365.57 | Used Time: 143.13\n",
      "Used Step: 83000 | Mean ep 100 return:  365.57 | Used Time: 144.86\n",
      "Used Step: 84000 | Mean ep 100 return:  365.57 | Used Time: 146.56\n",
      "Used Step: 85000 | Mean ep 100 return:  365.57 | Used Time: 148.26\n",
      "Used Step: 86000 | Mean ep 100 return:  365.57 | Used Time: 150.0\n",
      "Used Step: 87000 | Mean ep 100 return:  380.48 | Used Time: 151.75\n",
      "Used Step: 88000 | Mean ep 100 return:  368.53 | Used Time: 153.51\n",
      "Used Step: 89000 | Mean ep 100 return:  368.53 | Used Time: 155.26\n",
      "Used Step: 90000 | Mean ep 100 return:  368.53 | Used Time: 157.01\n",
      "Used Step: 91000 | Mean ep 100 return:  370.55 | Used Time: 158.76\n",
      "Used Step: 92000 | Mean ep 100 return:  370.55 | Used Time: 160.6\n",
      "Used Step: 93000 | Mean ep 100 return:  368.78 | Used Time: 162.39\n",
      "Used Step: 94000 | Mean ep 100 return:  369.42 | Used Time: 164.16\n",
      "Used Step: 95000 | Mean ep 100 return:  369.42 | Used Time: 165.93\n",
      "Used Step: 96000 | Mean ep 100 return:  365.76 | Used Time: 167.76\n",
      "Used Step: 97000 | Mean ep 100 return:  365.76 | Used Time: 169.54\n",
      "Used Step: 98000 | Mean ep 100 return:  365.03 | Used Time: 171.34\n",
      "Used Step: 99000 | Mean ep 100 return:  365.03 | Used Time: 173.14\n",
      "Used Step: 100000 | Mean ep 100 return:  365.97 | Used Time: 174.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sac = SAC()\n",
    "\n",
    "# model load with check\n",
    "if LOAD:\n",
    "    sac.load_model()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "# env reset\n",
    "s = env.reset()\n",
    "s = np.array(s)\n",
    "\n",
    "for step in tqdm_notebook(range(1, int(1e+5)+1)):\n",
    "    \n",
    "    a = sac.choose_action(s)\n",
    "    \n",
    "    # take action and get next state\n",
    "    s_, r, done, infos = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    \n",
    "    # log arrange\n",
    "    for info in infos:\n",
    "        maybeepinfo = info.get('episode')\n",
    "        if maybeepinfo: epinfobuf.append(maybeepinfo)\n",
    "            \n",
    "    # store transition\n",
    "    for i in range(len(s_)):\n",
    "        sac.store_transition(s[i],a[i],r[i],s_[i], done[i])\n",
    "        \n",
    "#     if (step >= LEARN_START) and (step % LEARN_FREQ == 0):\n",
    "#         sac.learn()\n",
    "        \n",
    "    s = s_\n",
    "            \n",
    "    if step % LOG_FREQ == 0:\n",
    "        # print log and save\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        # print epi log\n",
    "        print('Used Step:',sac.memory_counter,\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '| Used Time:',time_interval)\n",
    "        # save model\n",
    "        if SAVE:\n",
    "            sac.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open( \"replay.pkl\", \"wb\" ) as f:\n",
    "    pickle.dump(sac.replay_buffer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"replay.pkl\", \"rb\" ) as f:\n",
    "    replay = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "        b_next_state_memory, b_done = replay.sample(4)\n",
    "print(b_state_memory.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym_tictactoe\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# 3d tic-tac-toe in openai gym\n",
    "# https://pypi.org/project/gym-tictactoe/\n",
    "# pip install gym-tictactoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-04 16:07:25,346] Making new env: tictactoe-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('tictactoe-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = '1021'\n",
    "'''\n",
    "1: player 1\n",
    "0: first axis\n",
    "2: second axis\n",
    "1: third axis\n",
    "'''\n",
    "s_, r, done, info = env.step(action)\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 0, 0], [0, 0, 0], [0, 1, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0]]]\n",
      "0\n",
      "False\n",
      "{'round': 1, 'next_player': 2}\n"
     ]
    }
   ],
   "source": [
    "print(s_)\n",
    "print(r)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def state2string(state):\n",
    "#     return ''.join(str(j) for rows in state for row in rows for j in row)\n",
    "\n",
    "# def state2pos(state):\n",
    "#     # state to possible actions\n",
    "#     return [''.join(str(i) for i in list(ind)) \\\n",
    "#             for ind in np.argwhere(np.array(state)==0)]\n",
    "\n",
    "def state2string(state):\n",
    "    # care abount only first axis\n",
    "    return ''.join(str(j) for ind in state[0] for j in ind)\n",
    "\n",
    "def state2pos(state):\n",
    "    # state to possible actions\n",
    "    return ['0'+''.join(str(i) for i in list(ind)) \\\n",
    "            for ind in np.argwhere(np.array(state[0])==0)]\n",
    "\n",
    "def state2traj(state):\n",
    "    # state to episode trajectory\n",
    "    traj = [0 for i in range(len(np.argwhere(np.array(state)!=0)))]\n",
    "    for i, ind in enumerate(np.argwhere(np.array(state)==1)):\n",
    "        traj[2*i] = '1'+''.join(str(i) for i in list(ind))\n",
    "    for i, ind in enumerate(np.argwhere(np.array(state)==2)):\n",
    "        traj[2*i+1] = '2'+''.join(str(i) for i in list(ind))\n",
    "    return traj\n",
    "\n",
    "def give_env(state):\n",
    "    e = gym.make('tictactoe-v0')\n",
    "    s = e.reset()\n",
    "    traj = state2traj(state)\n",
    "    for t in traj:\n",
    "        e.step(t)\n",
    "    return e\n",
    "\n",
    "def one_step(state, action):\n",
    "    # give one step simulation\n",
    "    e = give_env(state)\n",
    "    s_, r, done, info = e.step(action)\n",
    "    s_ = deepcopy(s_)\n",
    "    return s_, r, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-04 16:07:25,733] Making new env: tictactoe-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000010\n",
      "['000', '001', '002', '010', '011', '012', '020', '022']\n",
      "['1021']\n",
      "o - -    - - -    - - -    \n",
      "- - x    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "([[[2, 0, 0], [0, 0, 0], [0, 1, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 0]]], 0, False)\n"
     ]
    }
   ],
   "source": [
    "print(state2string(s_))\n",
    "print(state2pos(s_))\n",
    "print(state2traj(s_))\n",
    "print(one_step(s_,'2'+state2pos(s_)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # node class for each state\n",
    "    def __init__(self, player, state, parent=None, done=False):\n",
    "        self.player = player # 1 or 2\n",
    "        self.parent = parent # node instance \n",
    "        self.state = state[:] # state to play. shape : (3, 3, 3)\n",
    "        self.done = done\n",
    "        self.state_str = state2string(self.state)\n",
    "        self.pos = state2pos(self.state) # possible actions\n",
    "        self.counts = np.array([0 for i in range(len(self.pos))]) # values for possible actions\n",
    "        self.values = np.array([0 for i in range(len(self.pos))]) # counts for possible actions\n",
    "        \n",
    "    def best_action(self, c=5.):\n",
    "        ucb = self.values/self.counts + c * np.sqrt(2*np.log(np.sum(self.counts))/self.counts)\n",
    "        ind = np.random.choice(np.argwhere(ucb == np.max(ucb)).flatten().tolist())\n",
    "        action = self.pos[ind]\n",
    "        return ind, str(self.player) + action, self.values/self.counts, self.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Node(2, s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000010\n",
      "['000', '001', '002', '010', '011', '012', '020', '022']\n",
      "[0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(n.state_str)\n",
    "print(n.pos)\n",
    "print(n.counts)\n",
    "print(n.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCT:\n",
    "    def __init__(self, player, state, iter_num=200):\n",
    "        self.player = player\n",
    "        self.root = Node(self.player, state) # root node of tree\n",
    "        self.iter_num = iter_num # computational budget\n",
    "        self.node_list = [self.root] # tree nodes\n",
    "        \n",
    "    def uct_search(self, c):\n",
    "        # collect trajectory you can fix this with conditional loop\n",
    "        for i in range(self.iter_num):\n",
    "            new_node, traj, r = self.tree_policy(c)\n",
    "            if new_node is not None:\n",
    "                if not(new_node.done) and len(state2pos(new_node.state))>0:\n",
    "                    r = self.default_policy(new_node)\n",
    "            self.backup(traj, r)\n",
    "        return self.root.best_action(c=0.0)\n",
    "            \n",
    "    def tree_policy(self, c):\n",
    "        # index of node_list for episode trajectory\n",
    "        node = self.root\n",
    "        traj = []\n",
    "        r = 0\n",
    "        while not(node.done) and len(node.pos)>0:\n",
    "            # if there exists unchosen action do it.\n",
    "            # note that every tree_policy call makes expand call until ariive to leaf node\n",
    "            if len(np.argwhere(node.counts==0))>0:\n",
    "                new_node, ind, r = self.expand(node)\n",
    "                traj.append((node,ind))\n",
    "                if new_node is not None:\n",
    "                    self.node_list.append(new_node)\n",
    "                break\n",
    "            # else choose best action\n",
    "            else:\n",
    "                ind, a, v, counts = node.best_action(c)\n",
    "                state, r, done = one_step(node.state, a)\n",
    "                if len(state2pos(deepcopy(state)))==0:\n",
    "                    done = True\n",
    "                if not(done):\n",
    "                    # if not done make state for new node\n",
    "                    e = give_env(state)\n",
    "                    action = np.random.choice(state2pos(state))\n",
    "                    s_, r, done, info = e.step(str(3-self.player)+action)\n",
    "                    r = -r\n",
    "                    state = deepcopy(s_)\n",
    "                    e.close()\n",
    "                    for n in self.node_list:\n",
    "                        # if seen state stop \n",
    "                        if n.state == state:\n",
    "                            new_node = n\n",
    "                            traj.append((node, ind))\n",
    "                            break\n",
    "                    else:\n",
    "                        # else add node to node_list\n",
    "                        new_node = Node(self.player, state, parent=node, done=done)\n",
    "                        traj.append((node,ind))\n",
    "                        self.node_list.append(new_node)\n",
    "                else:\n",
    "                    # if done \n",
    "                    new_node = None\n",
    "                    traj.append((node, ind))\n",
    "                    break\n",
    "            node = new_node\n",
    "        return new_node, traj, r\n",
    "            \n",
    "    def expand(self, node):\n",
    "        # only call when there exists zero-chosen action in the node\n",
    "        assert len(np.argwhere(node.counts==0))>0\n",
    "        # choose zero-chosen action\n",
    "        ind = np.argwhere(node.counts==0)[0][0]\n",
    "        action = node.pos[ind]\n",
    "        e = give_env(node.state)\n",
    "        s_, r, done, info = e.step(str(node.player)+action)\n",
    "        if len(state2pos(s_))==0:\n",
    "            done = True\n",
    "        if not(done):\n",
    "            # give next node whose player is itself\n",
    "            # note that reward is negative of player's reward\n",
    "            s_, r, done, info = e.step(str(3-node.player)+np.random.choice(state2pos(deepcopy(s_))))\n",
    "            new_node = Node(self.player, deepcopy(s_), parent=node, done=done)\n",
    "            r = -r\n",
    "            e.close()\n",
    "            return node, ind, r\n",
    "        else:\n",
    "            return None, ind, r\n",
    "    \n",
    "    def default_policy(self, node):\n",
    "        done = node.done\n",
    "        player = node.player\n",
    "        s = node.state\n",
    "        e = give_env(s)\n",
    "        r = 0\n",
    "        if len(state2pos(node.state))==0:\n",
    "            done = True\n",
    "        while not(done):\n",
    "            action = np.random.choice(state2pos(node.state))\n",
    "            s_, r, done, info = e.step(str(player)+action)\n",
    "            player = 3 - player # 1 -> 2, 2 -> 1\n",
    "            s = deepcopy(s_)\n",
    "        if player == node.player:\n",
    "            # lose\n",
    "            r *= -1\n",
    "        e.close()\n",
    "        return r\n",
    "    \n",
    "    def backup(self, traj, r):\n",
    "        for (node,ind) in traj:\n",
    "            node.counts[ind] += 1\n",
    "            node.values[ind] += r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('tictactoe-v0')\n",
    "s = env.reset()\n",
    "player = 1\n",
    "c = 5.0\n",
    "done = False\n",
    "traj = []\n",
    "while not(done) and len(state2pos(s))>0:\n",
    "    u = UCT(player, s)\n",
    "    ind, a, value, counts = u.uct_search(c)\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s = deepcopy(s_)\n",
    "    player = 3 - player # 1 -> 2, 2 -> 1\n",
    "    # anneal the exploration rate\n",
    "    c *= 0.9\n",
    "    traj.append((s, value, counts, r, done, info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[[0, 1, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([-0.05263158,  0.46153846,  0.33333333,  0.30434783,  0.33333333,\n",
       "         -0.05263158,  0.14285714,  0.25      ,  0.        ]),\n",
       "  array([19, 26, 24, 23, 24, 19, 21, 24, 20]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 2, 'round': 1}),\n",
       " ([[[0, 1, 0], [0, 2, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([-0.4       ,  0.11111111, -0.18181818,  0.2       ,  0.        ,\n",
       "          0.11111111, -0.18181818,  0.        ]),\n",
       "  array([20, 27, 22, 30, 26, 27, 22, 26]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 1, 'round': 2}),\n",
       " ([[[0, 1, 1], [0, 2, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([ 0.33333333,  0.44444444, -0.23809524,  0.03703704,  0.03703704,\n",
       "          0.14285714,  0.14285714]),\n",
       "  array([33, 36, 21, 27, 27, 28, 28]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 2, 'round': 3}),\n",
       " ([[[2, 1, 1], [0, 2, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([ 0.47272727,  0.03030303,  0.05882353, -0.21428571, -0.39130435,\n",
       "         -0.18518519]),\n",
       "  array([55, 33, 34, 28, 23, 27]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 1, 'round': 4}),\n",
       " ([[[2, 1, 1], [0, 2, 0], [0, 0, 1]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([-0.20588235,  0.04347826, -0.58333333, -0.56      ,  0.35211268]),\n",
       "  array([34, 46, 24, 25, 71]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 2, 'round': 5}),\n",
       " ([[[2, 1, 1], [0, 2, 2], [0, 0, 1]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([ 0.36666667,  0.57471264, -0.12903226, -0.45454545]),\n",
       "  array([60, 87, 31, 22]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 1, 'round': 6}),\n",
       " ([[[2, 1, 1], [1, 2, 2], [0, 0, 1]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([-0.00884956, -0.51162791, -0.5       ]),\n",
       "  array([113,  43,  44]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 2, 'round': 7}),\n",
       " ([[[2, 1, 1], [1, 2, 2], [2, 0, 1]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([ 0.00970874, -0.01030928]),\n",
       "  array([103,  97]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 1, 'round': 8}),\n",
       " ([[[2, 1, 1], [1, 2, 2], [2, 1, 1]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
       "   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],\n",
       "  array([0.]),\n",
       "  array([200]),\n",
       "  0,\n",
       "  False,\n",
       "  {'next_player': 2, 'round': 9})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from replay_memory import ReplayBuffer, PrioritizedReplayBuffer\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "# 만약 opencv-python이 설치되어있지 않다면 다음을 통해서 설치해주세요.\n",
    "# pip install opencv-python\n",
    "# 만약 설치에 오류가 발생한다면 다음을 참고해주세요.\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_table_of_contents_setup/py_table_of_contents_setup.html#py-table-of-content-setup\n",
    "from wrappers import wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU: True\n"
     ]
    }
   ],
   "source": [
    "'''DQN settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# target policy sync interval\n",
    "TARGET_REPLACE_ITER = 1\n",
    "# simulator steps for start learning\n",
    "LEARN_START = 10**2\n",
    "# (prioritized) experience replay memory size\n",
    "MEMORY_CAPACITY = 10**4\n",
    "# simulator steps for learning interval\n",
    "LEARN_FREQ = 1\n",
    "# check per\n",
    "is_per = True\n",
    "# alpha of PER\n",
    "PER_ALPHA = 0.6\n",
    "PER_BETA = 0.4\n",
    "PER_EPSILON = 1e-6\n",
    "# Double DQN\n",
    "DOUBLE = True\n",
    "# Dueling architecture\n",
    "DUEL = True\n",
    "\n",
    "'''Environment Settings'''\n",
    "# openai gym env name \n",
    "# https://gym.openai.com/envs/CartPole-v1/\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "env = gym.make(ENV_NAME)\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape\n",
    "# Total simulation step\n",
    "STEP_NUM = 10**5\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# epsilon-greedy\n",
    "EPSILON = 0.0\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# save frequency\n",
    "SAVE_FREQ = 10**4\n",
    "# paths for predction net, target net, result log\n",
    "PRED_PATH = './data/model/pred_net_simple.pkl'\n",
    "TARGET_PATH = './data/model/target_net_simple.pkl'\n",
    "RESULT_PATH = './data/plots/result_simple.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-6a9a0a0a8f31>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-6a9a0a0a8f31>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    nn.ReLU(),\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # nn.Sequential을 사용하면 다음과 같입 코드를 간결하게 바꿀 수 있습니다.\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Linear(N_STATES[0] * STATE_LEN , 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if DUEL:\n",
    "            # advantage function/ state value function\n",
    "            self.fc_adv = nn.Linear(128, N_ACTIONS)\n",
    "            self.fc_val = nn.Linear(128, 1)\n",
    "        else:\n",
    "            # action value function\n",
    "            self.fc_q = nn.Linear(128, N_ACTIONS) \n",
    "            \n",
    "        # 파라미터 값 초기화 코드는 다음과 같이 간결하게 바꿀 수 있습니다.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x는 (m, 84, 84, 4)의 tensor\n",
    "        x = self.feature_extraction(x)\n",
    "        \n",
    "        if DUEL:\n",
    "            adv = self.fc_adv(x)\n",
    "            val = self.fc_val(x)\n",
    "            action_value = val + adv - adv.mean(1).unsqueeze(1)\n",
    "        else:\n",
    "            action_value = self.fc_q(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.pred_net, self.target_net = Net(), Net()\n",
    "        # sync eval target\n",
    "        self.update_target(self.target_net, self.pred_net, 1.0)\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.pred_net.cuda()\n",
    "            self.target_net.cuda()\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        # target network step counter\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # ceate the replay buffer\n",
    "        if is_per:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(MEMORY_CAPACITY, alpha=PER_ALPHA)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.pred_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "            \n",
    "    def save_model(self):\n",
    "        # save prediction network and target network\n",
    "        self.pred_net.save(PRED_PATH)\n",
    "        self.target_net.save(TARGET_PATH)\n",
    "\n",
    "    def load_model(self):\n",
    "        # load prediction network and target network\n",
    "        self.pred_net.load(PRED_PATH)\n",
    "        self.target_net.load(TARGET_PATH)\n",
    "\n",
    "    def choose_action(self, x, EPSILON):\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda()\n",
    "\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            # greedy case\n",
    "            action_value = self.pred_net(x.unsqueeze(0))\n",
    "            action = torch.argmax(action_value).data.cpu().numpy()\n",
    "        else:\n",
    "            # random exploration case\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "\n",
    "    def learn(self, beta):\n",
    "        self.learn_step_counter += 1\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.update_target(self.target_net, self.pred_net, 1e-2)\n",
    "        \n",
    "        # data sample from experience replay\n",
    "        if is_per:\n",
    "            experience = self.replay_buffer.sample(BATCH_SIZE, beta=beta)\n",
    "            (b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "             b_next_state_memory, b_done, b_weights, b_idxes) = experience\n",
    "        else:\n",
    "            b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "            b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            b_weights, b_idxes = np.ones_like(b_reward_memory), None\n",
    "            \n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_r = torch.FloatTensor(b_reward_memory)\n",
    "        b_s_ = torch.FloatTensor(b_next_state_memory)\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(), b_a.cuda(), b_r.cuda(), b_s_.cuda(), b_d.cuda()\n",
    "\n",
    "        # action value prediction\n",
    "        q_eval = self.pred_net(b_s).gather(1, b_a.unsqueeze(1)).view(-1)\n",
    "        # shape : (m)\n",
    "\n",
    "        if DOUBLE:\n",
    "            # get best actions of next state\n",
    "            _ , best_actions = self.pred_net(b_s_).detach().max(1)\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "            # shape (m)\n",
    "        else:\n",
    "            # get next state value\n",
    "            q_next = self.target_net(b_s_).detach()\n",
    "            # get target value\n",
    "            q_target = b_r + GAMMA *(1.-b_d)* q_next.max(1)[0]\n",
    "            # shape (m)\n",
    "            \n",
    "        # calc huber loss, dont reduce for importance weight\n",
    "        loss = F.smooth_l1_loss(q_eval, q_target, reduction='none')\n",
    "        # calc importance weighted loss\n",
    "        b_w = torch.Tensor(b_weights)\n",
    "        if USE_GPU:\n",
    "            b_w = b_w.cuda()\n",
    "        loss = torch.mean(b_w*loss)\n",
    "        # get td error\n",
    "        td_error = (q_target - q_eval).data.cpu().numpy()\n",
    "        \n",
    "        # update importance weight\n",
    "        if is_per:\n",
    "            new_priorities = np.abs(td_error) + PER_EPSILON\n",
    "            self.replay_buffer.update_priorities(b_idxes, new_priorities)\n",
    "        \n",
    "        # backprop loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.pred_net.parameters(),1.)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    dqn.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward (since we are using EpisodicLifeEnv of OpenAI gym wrapper)\n",
    "epi_step = 0\n",
    "# accumulate return of current episode\n",
    "entire_ep_r = 0.\n",
    "# log for accumulate returns\n",
    "entire_ep_rs = []\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "while dqn.memory_counter <= STEP_NUM:\n",
    "    # env reset\n",
    "    s = np.array(env.reset())\n",
    "    # concat states\n",
    "    s_cat = [s] * STATE_LEN\n",
    "    s_cat_ = [s] * STATE_LEN\n",
    "    \n",
    "    # initialize one episode reward\n",
    "    ep_r = 0.\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(np.reshape(np.array(s_cat),(N_STATES[0] * STATE_LEN)), EPSILON)\n",
    "\n",
    "        # take action and get next state\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_cat_.append(s_)\n",
    "        s_cat_.pop(0)\n",
    "\n",
    "        # accumulate return\n",
    "        ep_r += r\n",
    "        # clip rewards for numerical stability\n",
    "        if r >= 0:\n",
    "            clip_r = 0.\n",
    "        else:\n",
    "            clip_r = r\n",
    "\n",
    "        # store the transition\n",
    "        dqn.store_transition(np.reshape(np.array(s_cat),(N_STATES[0] * STATE_LEN)), \\\n",
    "                             a, clip_r, np.reshape(np.array(s_cat_),(N_STATES[0] * STATE_LEN)),\\\n",
    "                             float(done))\n",
    "\n",
    "        # annealing the epsilon(exploration strategy), beta(per smoothing)\n",
    "        if dqn.memory_counter <= MEMORY_CAPACITY:\n",
    "            # linear annealing to 0.9 until million step\n",
    "            EPSILON += 0.9/MEMORY_CAPACITY\n",
    "        elif dqn.memory_counter <= STEP_NUM:\n",
    "            # linear annealing to 0.99 until the end\n",
    "            EPSILON += 0.09/(STEP_NUM - MEMORY_CAPACITY)\n",
    "            # linear annealing to 1 until the end\n",
    "            PER_BETA += (1.0 - PER_BETA) /(STEP_NUM - MEMORY_CAPACITY)\n",
    "\n",
    "        # if memory fill 50K and mod 4 = 0(for speed issue), learn pred net\n",
    "        if (LEARN_START <= dqn.memory_counter) and (dqn.memory_counter % LEARN_FREQ == 0):\n",
    "            dqn.learn(PER_BETA)\n",
    "            \n",
    "        # print log and save\n",
    "        if dqn.memory_counter % SAVE_FREQ == 0:\n",
    "            # check time interval\n",
    "            time_interval = round(time.time() - start_time, 2)\n",
    "            # calc mean return\n",
    "            mean_10_ep_return = round(np.mean(entire_ep_rs[-10:-1]),2)\n",
    "            result.append(mean_10_ep_return)\n",
    "            # print log\n",
    "            print('Ep: ',epi_step,\n",
    "                  '| Mean ep 10 return: ', mean_10_ep_return,\n",
    "                  '/Used Time:',time_interval,\n",
    "                  '/Used Step:',dqn.memory_counter,\n",
    "                 'Epsilon: ',round(EPSILON, 2))\n",
    "            # save model\n",
    "            dqn.save_model()\n",
    "            pkl_file = open(RESULT_PATH, 'wb')\n",
    "            pickle.dump(np.array(result), pkl_file)\n",
    "            pkl_file.close()\n",
    "            print('Save complete!')\n",
    "            \n",
    "        # if agent meets end-of-life, update return, acc return\n",
    "        if done:\n",
    "            entire_ep_r += ep_r\n",
    "            epi_step += 1\n",
    "            if epi_step % 1 == 0:\n",
    "                entire_ep_rs.append(entire_ep_r)\n",
    "                entire_ep_r = 0.\n",
    "            break\n",
    "\n",
    "        s_cat = deepcopy(s_cat_)\n",
    "\n",
    "        if RENDERING:\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "plt.plot(range(len(entire_ep_rs)), entire_ep_rs)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./simple_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "s = env.reset()\n",
    "s_cat = [s] * STATE_LEN\n",
    "s_cat_ = [s] * STATE_LEN\n",
    "total_reward = 0\n",
    "frames = []\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a = dqn.choose_action(np.reshape(np.array(s_cat),(N_STATES[0] * STATE_LEN)), EPSILON)\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_cat_.append(s_)\n",
    "    s_cat_.pop(0)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        break\n",
    "    s_cat = deepcopy(s_cat_)\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./simple_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

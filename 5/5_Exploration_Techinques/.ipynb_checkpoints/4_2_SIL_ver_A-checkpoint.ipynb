{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import deque\n",
    "from replay_memory import ReplayBuffer, SilPrioritizedReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from wrappers import wrap, wrap_cover, SubprocVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_ACTIONS :  4\n",
      "N_STATES :  (4, 84, 84)\n",
      "USE GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/kaist_mlilab/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class 'wrappers.FrameStack'> doesn't implement 'reset' method, but it implements deprecated '_reset' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "'''SAC Settings'''\n",
    "# coefficient of entropy regularization\n",
    "ENT_COEF = 1e-2\n",
    "# experience replay memory size\n",
    "MEMORY_CAPACITY = 10**6\n",
    "# learn start\n",
    "LEARN_START = int(1e+3)\n",
    "# learn frequency\n",
    "LEARN_FREQ = 1\n",
    "\n",
    "'''Environment Settings'''\n",
    "# sequential images to define state\n",
    "STATE_LEN = 4\n",
    "# openai gym env name\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4'\n",
    "# number of environments for SAC\n",
    "N_ENVS = 4\n",
    "# define gym \n",
    "env = SubprocVecEnv([wrap_cover(ENV_NAME) for i in range(N_ENVS)])\n",
    "# check gym setting\n",
    "N_ACTIONS = env.action_space.n;print('N_ACTIONS : ',N_ACTIONS) #  4\n",
    "N_STATES = env.observation_space.shape;print('N_STATES : ',N_STATES) # (4, 84, 84)\n",
    "# Total simulation step\n",
    "N_STEP = 10**7\n",
    "# gamma for MDP\n",
    "GAMMA = 0.99\n",
    "# visualize for agent playing\n",
    "RENDERING = False\n",
    "\n",
    "'''Training settings'''\n",
    "# check GPU usage\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print('USE GPU: '+str(USE_GPU))\n",
    "# mini-batch size\n",
    "BATCH_SIZE = 32\n",
    "# learning rage\n",
    "LR = 1e-4\n",
    "# clip gradient\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "'''Save&Load Settings'''\n",
    "# log frequency\n",
    "LOG_FREQ = int(1e+3)\n",
    "# check save/load\n",
    "SAVE = True\n",
    "LOAD = False\n",
    "# paths for predction net, target net, result log\n",
    "ACTOR_PATH = './data/model/sil_actor_net.pkl'\n",
    "CRITIC_PATH = './data/model/sil_critic_net.pkl'\n",
    "ACTION_CRITIC_PATH = './data/model/sil_action_critic_net.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        # actor\n",
    "        self.actor = nn.Linear(512, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_log_prob = F.log_softmax(self.actor(x), dim=1)\n",
    "\n",
    "        return action_log_prob\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),ACTOR_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(ACTOR_PATH))\n",
    "        \n",
    "class CriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        # actor\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),CRITIC_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(CRITIC_PATH))\n",
    "        \n",
    "class ActionCriticConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionCriticConvNet, self).__init__()\n",
    "        # architecture def\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(STATE_LEN, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        # actor\n",
    "        self.action_critic = nn.Linear(512, N_ACTIONS)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain = np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of (m, 4, 84, 84)\n",
    "        x = self.feature_extraction(x / 255.0)\n",
    "        # x.size(0) : mini-batch size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        # use log_softmax for numerical stability\n",
    "        action_value = self.action_critic(x)\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save(self.state_dict(),ACTION_CRITIC_PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        self.load_state_dict(torch.load(ACTION_CRITIC_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC + SIL 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of this code is based on original SIL code \n",
    "# https://github.com/junhyukoh/self-imitation-learning/blob/master/baselines/common/self_imitation.py\n",
    "class SSAC:\n",
    "    def __init__(self):\n",
    "        self.actor_net = ActorConvNet()\n",
    "        self.critic_net = CriticConvNet()\n",
    "        self.critic_target = CriticConvNet()\n",
    "        self.action_critic_net = ActionCriticConvNet()\n",
    "        # use gpu\n",
    "        if USE_GPU:\n",
    "            self.actor_net = self.actor_net.cuda(device=3)\n",
    "            self.critic_net = self.critic_net.cuda(device=3)\n",
    "            # critic target network for stability\n",
    "            self.critic_target = self.critic_net.cuda(device=3)\n",
    "            self.action_critic_net = self.action_critic_net.cuda(device=3)\n",
    "        \n",
    "        # sync net and target\n",
    "        self.critic_target.load_state_dict(self.critic_net.state_dict())\n",
    "            \n",
    "        # simulator step conter\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # create running episode memory\n",
    "        self.running_episodes = [[] for _ in range(N_ENVS)]\n",
    "        \n",
    "        # Create the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "        self.sil_buffer = SilPrioritizedReplayBuffer(MEMORY_CAPACITY, 0.6)\n",
    "            \n",
    "        # define optimizer\n",
    "        self.actor_opt = torch.optim.Adam(self.actor_net.parameters(), lr=LR)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic_net.parameters(), lr=LR)\n",
    "        self.action_critic_opt = torch.optim.Adam(self.action_critic_net.parameters(), lr=LR)\n",
    "        \n",
    "    def update_target(self, target, pred, update_rate):\n",
    "        # update target network parameters using predcition network\n",
    "        for target_param, pred_param in zip(target.parameters(), pred.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) \\\n",
    "                                    * target_param.data + update_rate*pred_param.data)\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        \n",
    "        self.actor_net.save(ACTOR_PATH)\n",
    "        self.critic_net.save(CRITIC_PATH)\n",
    "        self.action_critic_net.save(ACTION_CRITIC_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=3)\n",
    "            self.critic_net.cuda(device=3)\n",
    "            self.action_critic_net.cuda(device=3)\n",
    "            \n",
    "    def load_model(self):\n",
    "        self.actor_net.cpu()\n",
    "        self.critic_net.cpu()\n",
    "        self.action_critic_net.cpu()\n",
    "        \n",
    "        self.actor_net.load(ACTOR_PATH)\n",
    "        self.critic_net.load(CRITIC_PATH)\n",
    "        self.action_critic_net.load(ACTION_CRITIC_PATH)\n",
    "        \n",
    "        if USE_GPU:\n",
    "            self.actor_net.cuda(device=3)\n",
    "            self.critic_net.cuda(device=3)\n",
    "            self.action_critic_net.cuda(device=3)\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        # Assume that x is a np.array of shape (nenvs, 4, 84, 84)\n",
    "        x = torch.FloatTensor(x)\n",
    "        if USE_GPU:\n",
    "            x = x.cuda(device=3)\n",
    "        # get action log probs and state values\n",
    "        action_log_prob = self.actor_net(x)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1).data.cpu().numpy()\n",
    "        # sample actions\n",
    "        action = np.array([np.random.choice(N_ACTIONS,p=action_prob[i]) for i in range(len(action_prob))])\n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        self.memory_counter += 1\n",
    "        self.replay_buffer.add(s, a, r, s_, float(done))\n",
    "        \n",
    "    def sil_store_transition(self, s, a, r, done):\n",
    "        # sil episode caching\n",
    "        for n in range(N_ENVS):\n",
    "            self.running_episodes[n].append([s[n], a[n], r[n]])\n",
    "\n",
    "        for n, d in enumerate(done):\n",
    "            if d:\n",
    "                self.update_buffer(self.running_episodes[n])\n",
    "                self.running_episodes[n] = []\n",
    "                \n",
    "    # SIL update buffer\n",
    "    def update_buffer(self, trajectory):\n",
    "        positive_reward = False\n",
    "        for (ob, a, r) in trajectory:\n",
    "            if r > 0:\n",
    "                positive_reward = True\n",
    "                break\n",
    "        if positive_reward:\n",
    "            self.add_episode(trajectory)\n",
    "                \n",
    "    def add_episode(self, trajectory):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        for (ob, action, reward) in trajectory:\n",
    "            obs.append(ob)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(False)\n",
    "        dones[len(dones)-1]=True\n",
    "        returns = self.discount_with_dones(rewards, dones, GAMMA)\n",
    "        for (ob, action, R) in list(zip(obs, actions, returns)):\n",
    "            self.sil_buffer.add(ob, action, R)\n",
    "            \n",
    "    def discount_with_dones(self, rewards, dones, GAMMA):\n",
    "        discounted = []\n",
    "        r = 0\n",
    "        for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "            r = reward + GAMMA*r*(1.-done) # fixed off by one bug\n",
    "            discounted.append(r)\n",
    "        return discounted[::-1]\n",
    "\n",
    "    def learn(self):\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # data sample from experience replay\n",
    "        b_state_memory, b_action_memory, b_reward_memory, \\\n",
    "        b_next_state_memory, b_done = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        b_weights, b_idxes = np.ones_like(b_reward_memory), None\n",
    "\n",
    "        b_s = torch.FloatTensor(b_state_memory)\n",
    "        b_a = torch.LongTensor(b_action_memory)\n",
    "        b_r = torch.FloatTensor(b_reward_memory)\n",
    "        b_s_ = torch.FloatTensor(b_next_state_memory)\n",
    "        b_d = torch.FloatTensor(b_done)\n",
    "\n",
    "        if USE_GPU:\n",
    "            b_s, b_a, b_r, b_s_, b_d = b_s.cuda(device=3), b_a.cuda(device=3), b_r.cuda(device=3), b_s_.cuda(device=3), b_d.cuda(device=3)\n",
    "\n",
    "        # forward calc\n",
    "        action_log_prob = self.actor_net(b_s)\n",
    "        action_prob = F.softmax(action_log_prob, dim=1)\n",
    "        action_log_prob = F.log_softmax(action_log_prob, dim=1)\n",
    "        cur_value = self.critic_net(b_s).squeeze(1)\n",
    "        next_value = self.critic_target(b_s_)\n",
    "        action_value = self.action_critic_net(b_s)\n",
    "\n",
    "        # critic loss. eq (5) in SAC paper\n",
    "        value_target = (action_value - ENT_COEF * action_log_prob).gather(1, b_a.unsqueeze(1)).squeeze(1)\n",
    "        critic_loss = 0.5 * F.smooth_l1_loss(cur_value, value_target.detach())\n",
    "\n",
    "        # action critic loss. eq (7), (8) in SAC paper\n",
    "        action_value_target = b_r + GAMMA * (1-b_d) * next_value.squeeze(1)\n",
    "        action_critic_loss = 0.5 * F.smooth_l1_loss(action_value.gather(1, \n",
    "            b_a.unsqueeze(1)).squeeze(1), action_value_target.detach())\n",
    "\n",
    "        # actor loss. eq (10) in SAC paper\n",
    "        actor_loss = torch.mean(action_prob*(action_log_prob \\\n",
    "            - F.log_softmax(action_value.detach()/ENT_COEF, dim=1)))\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        self.action_critic_opt.zero_grad()\n",
    "        action_critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.action_critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.action_critic_opt.step()\n",
    "\n",
    "        self.update_target(self.critic_target, self.critic_net, 1e-3)\n",
    "\n",
    "        # SIL update\n",
    "        experience = self.sil_buffer.sample(BATCH_SIZE, beta=0)\n",
    "        (sil_b_s, sil_b_a, sil_b_r, sil_b_weights, sil_b_idxes) = experience\n",
    "\n",
    "        sil_b_s = torch.FloatTensor(sil_b_s)\n",
    "        sil_b_a = torch.LongTensor(sil_b_a)\n",
    "        sil_b_r = torch.FloatTensor(sil_b_r)\n",
    "        sil_b_w = torch.Tensor(sil_b_weights)\n",
    "\n",
    "        if USE_GPU:\n",
    "            sil_b_s, sil_b_a, sil_b_r, sil_b_w = sil_b_s.cuda(device=3), sil_b_a.cuda(device=3), sil_b_r.cuda(device=3), sil_b_w.cuda(device=3)\n",
    "\n",
    "        # forward calc\n",
    "        sil_action_log_prob = self.actor_net(sil_b_s)\n",
    "        sil_action_log_prob = F.log_softmax(sil_action_log_prob, dim=1)\n",
    "        sil_cur_value = self.critic_net(sil_b_s).squeeze(1)\n",
    "        sil_action_value = self.action_critic_net(sil_b_s)\n",
    "        sil_adv = (torch.clamp(F.relu(sil_b_r - sil_cur_value), 0.0, 1.0)).data.cpu().numpy()\n",
    "\n",
    "        # actor loss. eq (2) in SIL paper\n",
    "        sil_actor_loss = torch.mean( sil_b_w * (-sil_action_log_prob.gather(1,\n",
    "        sil_b_a.unsqueeze(1)).squeeze(1) * torch.clamp(F.relu(sil_b_r - sil_cur_value.detach()),0.0,1.0)))\n",
    "\n",
    "        # critic loss. eq (3) in SIL paper\n",
    "        sil_critic_loss = F.relu(sil_b_r - sil_cur_value)\n",
    "        sil_critic_loss = 0.5 * torch.mean(sil_b_w * (F.smooth_l1_loss(sil_critic_loss,\n",
    "        torch.zeros_like(sil_critic_loss), reduction='none')))\n",
    "        \n",
    "        # action critic_loss. this is not implemented in SIL paper\n",
    "        sil_action_critic_loss = F.relu(sil_b_r \\\n",
    "                            - sil_action_value.gather(1, sil_b_a.unsqueeze(1)).squeeze(1))\n",
    "        sil_action_critic_loss =0.5 * torch.mean(sil_b_w * \\\n",
    "                (F.smooth_l1_loss(sil_action_critic_loss, torch.zeros_like(sil_action_critic_loss)\\\n",
    "                , reduction='none')))\n",
    "\n",
    "        self.actor_opt.zero_grad()\n",
    "        sil_actor_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.actor_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        self.critic_opt.zero_grad()\n",
    "        sil_critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        self.action_critic_opt.zero_grad()\n",
    "        sil_action_critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.action_critic_net.parameters(), MAX_GRAD_NORM)\n",
    "        self.action_critic_opt.step()\n",
    "\n",
    "        self.sil_buffer.update_priorities(sil_b_idxes, sil_adv)\n",
    "\n",
    "        return round(float(actor_loss), 4), round(float(critic_loss), 4), round(float(action_critic_loss), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize results!\n",
      "Collecting experience...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead5e0ac8d9240b59cf363d37b5fc2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Step: 4000 | Mean ep 100 return:  1.7 | Used Time: 8.59\n"
     ]
    }
   ],
   "source": [
    "sac = SSAC()\n",
    "\n",
    "# model load with check\n",
    "if LOAD and os.path.isfile(PRED_PATH) and os.path.isfile(TARGET_PATH):\n",
    "    sac.load_model()\n",
    "    pkl_file = open(RESULT_PATH,'rb')\n",
    "    result = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print('Load complete!')\n",
    "else:\n",
    "    result = []\n",
    "    print('Initialize results!')\n",
    "\n",
    "print('Collecting experience...')\n",
    "\n",
    "# episode step for accumulate reward \n",
    "epinfobuf = deque(maxlen=100)\n",
    "# check learning time\n",
    "start_time = time.time()\n",
    "\n",
    "# env reset\n",
    "s = np.array(env.reset())\n",
    "\n",
    "for step in tqdm_notebook(range(1, N_STEP//N_ENVS + 1)):\n",
    "    \n",
    "    a = sac.choose_action(s)\n",
    "    \n",
    "    # take action and get next state\n",
    "    s_, r, done, infos = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    \n",
    "    # log arrange\n",
    "    for info in infos:\n",
    "        maybeepinfo = info.get('episode')\n",
    "        if maybeepinfo: epinfobuf.append(maybeepinfo)\n",
    "            \n",
    "    # store transition\n",
    "    sac.sil_store_transition(s,a,r,done)\n",
    "    for i in range(len(s_)):\n",
    "        sac.store_transition(s[i],a[i],r[i],s_[i], done[i])\n",
    "        \n",
    "    if (step >= LEARN_START) and (step % LEARN_FREQ == 0):\n",
    "        sac.learn()\n",
    "        \n",
    "    s = s_\n",
    "            \n",
    "    if step % LOG_FREQ == 0:\n",
    "        # print log and save\n",
    "        # check time interval\n",
    "        time_interval = round(time.time() - start_time, 2)\n",
    "        # calc mean return\n",
    "        mean_100_ep_return = round(np.mean([epinfo['r'] for epinfo in epinfobuf]),2)\n",
    "        result.append(mean_100_ep_return)\n",
    "        # print epi log\n",
    "        print('Used Step:',sac.memory_counter,\n",
    "              '| Mean ep 100 return: ', mean_100_ep_return,\n",
    "              '| Used Time:',time_interval)\n",
    "        # save model\n",
    "        if SAVE:\n",
    "            sac.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(result)), result)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "        \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "    anim.save('./ssac_breakout_result.gif', writer='imagemagick', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap(gym.make('BreakoutNoFrameskip-v4'))\n",
    "s = np.array(env.reset())\n",
    "total_reward = 0\n",
    "frames = []\n",
    "done_stack = 0\n",
    "\n",
    "for t in range(10000):\n",
    "    # Render into buffer. \n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    a = ssac.choose_action(np.expand_dims(s,axis=0))\n",
    "    # take action and get next state\n",
    "    s_, r, done, info = env.step(a)\n",
    "    s_ = np.array(s_)\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        done_stack += 1\n",
    "        if done_stack == 5:\n",
    "            break\n",
    "    s = s_\n",
    "env.close()\n",
    "print('Total Reward : %.2f'%total_reward)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./ppo_pong_result.gif \"segment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# 5주차 PyTorch로 시작하는 강화학습 입문 Camp

**출석 : 5명** (조교 2명 제외) ... 6명 

------

#### 커리큘럼

<span style="color:magenta">:: 금 주 진도 </span>

##### **Part 1. Basics**

> > <u>2017년 이전 까지의 연구를 "기초"라 표현</u>

-  강화학습 의 기반이 된 아이디어들 MDP, ML
-  가치 기반 방법론들 (Q-Learning, DQN)
-  정책 기반 방법론들(TRPO, PPO)

###### **Part 2. Advanced (다양한 주제)**

- Off-policy 정책기반 방법론들 (ACER, SAC)
- 탐험 기법들 (엔트로피 정규화, 내적 동기 부여)
- **강화학습과 불확실성 (Distributional DQN)**

###### **Part 3. Applications(적용의 확장)**

- 모방학습, 역강화 학습 (GAIL, IRL)
- 다중 에이전트 강화 학습(Regret, MCTS)

#### 피드백 

------

- 질문 많이해주세요.
- 추후 공부할 내용을 위해 라틴어도 알려주시는 친절함 
- 강의는 5시 41분에 종료되었습니다. 
- 

#### 강의 내용 

------

Policy iteration -Policy gradient, Actor critic  

Value iteration - Q-learning, DQN







#### 질문

---

MCBN - 색있는 부분의 면적은 실제로 무슨 의미? - eval 시에 표준편차*1 는 짙은 색, 표준편차*2는 연한색으로 색칠

평균만 알려주는 것 -37:30



기존 통계학에서는 모달이 여러개 있으면 여러개의 분포로 설명하려고 하는데 여기서는 꼭 왜 하나로 만들어서 하는가? 

- 예로 가우시안믹스쳐 모델링 하더도 다양성을 확보하지 못한 경우가 많다. 그리고  머신러닝 입장에서는 하나의 모델을 사용할 수 있으니까 그렇다. 

Distributed RL은 value의 멀티모달 분포를 학습하는 것이다.



표에서 ??? 는?

- 아직 특별히 도미넌트한 연구가 없다. 굳이 보려면 model_based RL

   

상태를 고정시킨다. 근데 상태는 너무 많지 않나? 

- 상태에서 행동을 했을 때 지금까지는 평균을 냈는데, 여기서는 각 상태 행동 쌍에 대해 분포를 보겠다는 것이다. 



평균을 - 17:00



Policy Gradient - 21:00



45:00



tau 가 input



퀀타일 미드 타운도 값인가? 그렇다.



 vectorizing 한 거 코드가 수도코드에는 , 계속 변하는 값 아닌가? 1:49:00



다구간 로스들의 평균을 내는 것인가? 

- 그렇다. 

